{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "# If batch_first = True then the input and output tensors are (batch, seq, feature)\n",
    "# Otherwise it's, (seq, batch, feature )\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5])\n",
      "Hidden state shape: torch.Size([1, 1, 10])\n",
      "Cell state shape torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "# Randomise an input tensor\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Initialise the hidden state\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "\n",
    "# Initialise the cell state\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "\n",
    "# Make a tuple of hidden and cell states\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "print(\"Input shape:\",inp.shape)\n",
    "print(\"Hidden state shape:\", hidden_state.shape)\n",
    "print(\"Cell state shape\", cell_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0964,  0.4761,  1.5061,  1.3182, -0.5897]]])\n"
     ]
    }
   ],
   "source": [
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.3679,  0.9422,  0.8958, -1.6662,  0.7875, -0.7595, -0.8883,\n",
      "           0.8247, -1.0505,  1.7329]]]), tensor([[[ 0.7525, -0.2488, -1.0012,  1.6008, -2.7763, -0.7171, -0.7462,\n",
      "          -1.6271,  0.6473, -0.2141]]]))\n"
     ]
    }
   ],
   "source": [
    "# Remember this is a tuple of hidden_state and cell_state\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1810, -0.2740, -0.3724,  0.2596, -0.0844, -0.2676, -0.3954,\n",
      "          -0.1689,  0.0517, -0.2117]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass\n",
    "# Remember hidden is a tuple of hidden_state and cell_state\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1810, -0.2740, -0.3724,  0.2596, -0.0844, -0.2676, -0.3954,\n",
      "          -0.1689,  0.0517, -0.2117]]], grad_fn=<StackBackward>), tensor([[[ 0.3271, -0.3798, -0.6802,  0.3936, -0.3623, -0.7226, -0.7636,\n",
      "          -0.6220,  0.0659, -0.4625]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Output shape is the same as the hidden state\n",
    "# Also note that the output is in fact the hidden_state\n",
    "# This is because we have a sequence length of 1\n",
    "\n",
    "print(\"Output shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5])\n",
      "torch.Size([1, 3, 10])\n",
      "torch.Size([1, 1, 10])\n",
      "torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Now increase the sequence length and see what happens\n",
    "seq_len = 3\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "print(inp.shape)\n",
    "\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "\n",
    "# The output will have the same sequence length as the input - you get an output for each input in sequence\n",
    "print(out.shape)\n",
    "\n",
    "# The cell hidden and cell states are still the same dimension as before\n",
    "print(hidden[0].shape)\n",
    "print(hidden[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0125, -0.0631, -0.1049,  0.3259, -0.0635, -0.3160, -0.1554,\n",
      "          -0.2281,  0.0431, -0.0153],\n",
      "         [-0.1226,  0.0285,  0.0116,  0.2910, -0.0378, -0.2754, -0.0009,\n",
      "          -0.1127,  0.0512, -0.0146],\n",
      "         [-0.1392,  0.0695,  0.0348,  0.2763,  0.1415, -0.2827,  0.0290,\n",
      "          -0.0816,  0.1464, -0.0390]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([-0.1392,  0.0695,  0.0348,  0.2763,  0.1415, -0.2827,  0.0290, -0.0816,\n",
      "         0.1464, -0.0390], grad_fn=<SliceBackward>)\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the last output - this is a many to one scenario where we just take the last output of the sequence\n",
    "print(out)\n",
    "out = out.squeeze()[-1, :]\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1392,  0.0695,  0.0348,  0.2763,  0.1415, -0.2827,  0.0290, -0.0816,\n",
      "         0.1464, -0.0390], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([100, 1000, 5])\n",
      "Hidden state shape: torch.Size([2, 100, 10])\n",
      "Cell state shape torch.Size([2, 100, 10])\n"
     ]
    }
   ],
   "source": [
    "# Now let's imagine a stock market example of what the data & topology would look like\n",
    "input_dim = 5  # This is the number of features at each time step\n",
    "hidden_dim = 10  # Hidden dimension\n",
    "n_layers = 2  # How deep is the model\n",
    "\n",
    "# If batch_first = True then the input and output tensors are (batch, seq, feature)\n",
    "# Otherwise it's, (seq, batch, feature )\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "# 100 different sequences in the training batch\n",
    "# Perhaps a rolling window of training examples?\n",
    "# Perhaps 100 sequences randonly sampled in a time period (with or without overlap allowed)\n",
    "batch_size = 100  \n",
    "seq_len = 1000  # 1000 consecutive days of data for each training sequence\n",
    "\n",
    "# Then the training label could be:\n",
    "# The absolute value on the 1001th day (would need to standardise / normalise the data for this to generalise?)\n",
    "# The sign of the value on the 1001th data vs the 1000th day (binary)?\n",
    "\n",
    "# Randomise an input tensor\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Initialise the hidden state\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "\n",
    "# Initialise the cell state\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "\n",
    "# Make a tuple of hidden and cell states\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "print(\"Input shape:\",inp.shape)\n",
    "print(\"Hidden state shape:\", hidden_state.shape)\n",
    "print(\"Cell state shape\", cell_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\q2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "\n",
    "# Regular expressions toolkit\n",
    "import re\n",
    "\n",
    "# Natural language toolkit\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# punkt is a pre-trained tokenizer in English. Divides text into a list of sentences using an unsupervised algo\n",
    "# to build a model for abbreviationof words, collocations and words that start sentences.\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_file = bz2.BZ2File('../data/amazon_reviews/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('../data/amazon_reviews/test.ft.txt.bz2')\n",
    "\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
    "num_test = 200000  # Using 200,000 reviews from test set\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n",
      "__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_file[0])\n",
    "print(test_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2\n",
      "stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^\n",
      "Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
     ]
    }
   ],
   "source": [
    "print(train_file[0].split(' ')[0])\n",
    "print(train_file[0].split(' ', 1)[1][:-1].lower())\n",
    "print(train_file[0].split(' ', 1)[1][:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels from sentences\n",
    "\n",
    "# Assumes the label is everything up to the first space in the review. The split actually splits the sentence into a \n",
    "# list - each element of the list is a word / character seq.  The zeroeth element in the list is the label \n",
    "# '__label__2' (positive) or # or '__label__1' (negative)\n",
    "\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "\n",
    "# .split(' ',1) means split the review into the first word (the label) and everything else. .split(' ',2) would split the\n",
    "# review into '__label__2' , 'Stuning' and everything else.\n",
    "# Then the index slice [1][:-1] accesses this 2nd part [1] and all the characters [:-1] and converst them all to lower case\n",
    "\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "# Do exactly the same for the test data\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batteries died within a year ...: i bought this charger in jul 2003 and it worked ok for a while. the design is nice and convenient. however, after about a year, the batteries would not hold a charge. might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
      "batteries died within a year ...: i bought this charger in jul 0000 and it worked ok for a while. the design is nice and convenient. however, after about a year, the batteries would not hold a charge. might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n"
     ]
    }
   ],
   "source": [
    "# Some simple cleaning of data\n",
    "# This replaces any numbers with the character '0'\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "print(test_sentences[2])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "print(test_sentences[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^\n",
      "stuning\n",
      "even\n",
      "for\n",
      "the\n",
      "non-gamer\n",
      ":\n",
      "this\n",
      "sound\n",
      "track\n",
      "was\n",
      "beautiful\n",
      "!\n",
      "it\n",
      "paints\n",
      "the\n",
      "senery\n",
      "in\n",
      "your\n",
      "mind\n",
      "so\n",
      "well\n",
      "i\n",
      "would\n",
      "recomend\n",
      "it\n",
      "even\n",
      "to\n",
      "people\n",
      "who\n",
      "hate\n",
      "vid\n",
      ".\n",
      "game\n",
      "music\n",
      "!\n",
      "i\n",
      "have\n",
      "played\n",
      "the\n",
      "game\n",
      "chrono\n",
      "cross\n",
      "but\n",
      "out\n",
      "of\n",
      "all\n",
      "of\n",
      "the\n",
      "games\n",
      "i\n",
      "have\n",
      "ever\n",
      "played\n",
      "it\n",
      "has\n",
      "the\n",
      "best\n",
      "music\n",
      "!\n",
      "it\n",
      "backs\n",
      "away\n",
      "from\n",
      "crude\n",
      "keyboarding\n",
      "and\n",
      "takes\n",
      "a\n",
      "fresher\n",
      "step\n",
      "with\n",
      "grate\n",
      "guitars\n",
      "and\n",
      "soulful\n",
      "orchestras\n",
      ".\n",
      "it\n",
      "would\n",
      "impress\n",
      "anyone\n",
      "who\n",
      "cares\n",
      "to\n",
      "listen\n",
      "!\n",
      "^_^\n",
      "['stuning', 'even', 'for', 'the', 'non-gamer', ':', 'this', 'sound', 'track', 'was', 'beautiful', '!', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n",
      "Counter({'the': 5, 'it': 5, '!': 4, 'i': 3, 'even': 2, 'would': 2, 'to': 2, 'who': 2, '.': 2, 'game': 2, 'music': 2, 'have': 2, 'played': 2, 'of': 2, 'and': 2, 'stuning': 1, 'for': 1, 'non-gamer': 1, ':': 1, 'this': 1, 'sound': 1, 'track': 1, 'was': 1, 'beautiful': 1, 'paints': 1, 'senery': 1, 'in': 1, 'your': 1, 'mind': 1, 'so': 1, 'well': 1, 'recomend': 1, 'people': 1, 'hate': 1, 'vid': 1, 'chrono': 1, 'cross': 1, 'but': 1, 'out': 1, 'all': 1, 'games': 1, 'ever': 1, 'has': 1, 'best': 1, 'backs': 1, 'away': 1, 'from': 1, 'crude': 1, 'keyboarding': 1, 'takes': 1, 'a': 1, 'fresher': 1, 'step': 1, 'with': 1, 'grate': 1, 'guitars': 1, 'soulful': 1, 'orchestras': 1, 'impress': 1, 'anyone': 1, 'cares': 1, 'listen': 1, '^_^': 1})\n",
      "0.0% done\n",
      "2.5% done\n",
      "5.0% done\n",
      "7.5% done\n",
      "10.0% done\n",
      "12.5% done\n",
      "15.0% done\n",
      "17.5% done\n",
      "20.0% done\n",
      "22.5% done\n",
      "25.0% done\n",
      "27.5% done\n",
      "30.0% done\n",
      "32.5% done\n",
      "35.0% done\n",
      "37.5% done\n",
      "40.0% done\n",
      "42.5% done\n",
      "45.0% done\n",
      "47.5% done\n",
      "50.0% done\n",
      "52.5% done\n",
      "55.0% done\n",
      "57.5% done\n",
      "60.0% done\n",
      "62.5% done\n",
      "65.0% done\n",
      "67.5% done\n",
      "70.0% done\n",
      "72.5% done\n",
      "75.0% done\n",
      "77.5% done\n",
      "80.0% done\n",
      "82.5% done\n",
      "85.0% done\n",
      "87.5% done\n",
      "90.0% done\n",
      "92.5% done\n",
      "95.0% done\n",
      "97.5% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    \n",
    "    if i == 0:\n",
    "        print(i, train_sentences[i])\n",
    "        \n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    \n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        \n",
    "        if i == 0:\n",
    "            print(word)\n",
    "        \n",
    "        # Not sure why we're doing the case conversion again\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "                \n",
    "        train_sentences[i].append(word)\n",
    "        \n",
    "    if i==0:\n",
    "        print(train_sentences[i])\n",
    "        print(words)\n",
    "        \n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "        \n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242\n"
     ]
    }
   ],
   "source": [
    "print(words['stunning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 99, 13, 28, 1448, 4272, 57, 31, 10, 3, 40, 1781, 10, 85, 1730, 2, 5, 27, 907, 8, 11, 99, 16, 152, 6, 5, 140, 90, 9, 2, 68, 5, 122, 14, 7, 42, 1847, 9, 210, 58, 243, 108, 2, 7, 133, 1847, 46, 29316, 38, 2642, 14, 3, 2379, 2, 11, 99, 46, 18845, 160, 2, 934, 30, 0, 0, 6, 560, 46, 1285, 2, 31, 10, 160, 21, 2336, 4156, 2, 11, 12, 7, 3570, 14981, 99, 14, 28, 24, 2, 182, 102, 130, 147, 9, 239, 12, 46, 827, 58, 2, 2587, 5, 263, 11, 4, 72, 601, 444, 4, 579, 4, 416, 4, 153, 4, 1690, 4, 1255, 1816, 521, 31, 179, 33, 80, 18, 17, 829, 61, 32]\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "40\n",
      "great cd : my lovely pat\n"
     ]
    }
   ],
   "source": [
    "print(idx2word[40])\n",
    "print(word2idx['great'])\n",
    "print(words[40], words[99], words[13], words[28], words[1448], words[4272])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0    40    99    13    28  1448  4272    57    31    10     3\n",
      "    40  1781    10    85  1730     2     5    27   907     8    11    99\n",
      "    16   152     6     5   140    90     9     2    68     5   122    14\n",
      "     7    42  1847     9   210    58   243   108     2     7   133  1847\n",
      "    46 29316    38  2642    14     3  2379     2    11    99    46 18845\n",
      "   160     2   934    30     0     0     6   560    46  1285     2    31\n",
      "    10   160    21  2336  4156     2    11    12     7  3570 14981    99\n",
      "    14    28    24     2   182   102   130   147     9   239    12    46\n",
      "   827    58     2  2587     5   263    11     4    72   601   444     4\n",
      "   579     4   416     4   153     4  1690     4  1255  1816   521    31\n",
      "   179    33    80    18    17   829    61    32]\n"
     ]
    }
   ],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "# These comments keep referring to sentences but in fact it's multiple sentences inside a review\n",
    "# The term should be review not sentence\n",
    "\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "print(test_sentences[0])\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# This splits the test populaton (200k reviews) into 50% validation and 50% test set\n",
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "\n",
    "print(len(test_sentences))\n",
    "\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network includes a word embedding layer that is also trained\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 1000... Loss: 0.143612... Val Loss: 0.184799\n",
      "Validation loss decreased (inf --> 0.184799).  Saving model ...\n",
      "Epoch: 1/2... Step: 2000... Loss: 0.167584... Val Loss: 0.168625\n",
      "Validation loss decreased (0.184799 --> 0.168625).  Saving model ...\n",
      "Epoch: 2/2... Step: 3000... Loss: 0.137612... Val Loss: 0.171890\n",
      "Epoch: 2/2... Step: 4000... Loss: 0.105190... Val Loss: 0.169851\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.163\n",
      "Test accuracy: 93.746%\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
