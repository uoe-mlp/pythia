{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as web\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import plotly.colors\n",
    "from datetime import datetime\n",
    "import time\n",
    "import datetime as dt\n",
    "import copy\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(asset_name='SPY', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', force_yahoo=0, verbosity=0):\n",
    "        \n",
    "    # If the 'directory' doesn't exist, create it\n",
    "    if os.path.isdir(directory) == False:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    pathname = directory + '/' + asset_name + '_' + start_date + '.csv'\n",
    "    if os.path.isfile(pathname) == True and not force_yahoo:\n",
    "        if verbosity >= 1:\n",
    "            print('Loading from file: ', pathname)        \n",
    "        df = pd.read_csv(pathname, index_col='Date')\n",
    "    else:\n",
    "        if verbosity >= 1:\n",
    "            print('Downloading from Yahoo! - ', asset_name)\n",
    "        df = web.DataReader(asset_name, data_source='yahoo', start=start_date, end=stop_date)\n",
    "        df.to_csv(pathname)    \n",
    "\n",
    "    # Copy the (date) index to a Date field and make a new index which enumerates\n",
    "    # all the entries.\n",
    "    df.insert(0, 'Date', df.index)\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    df['Date']=pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic stock generator\n",
    "\n",
    "# We'll generate the Adj Close price as the primary (since this is the sequnce of labels we are training on)\n",
    "# The relationship of the other features to the Adj Close will be as follows:\n",
    "# Close = close_scale * Adj Close\n",
    "# Open = previous day's Close\n",
    "# Low = Open\n",
    "# High = Close\n",
    "# Volume = 0\n",
    "\n",
    "# source_asset_name - name of asset to model from\n",
    "# trend_list - list of tuples - (magnitude, days for a full cycle of this trend)\n",
    "\n",
    "def make_synthetic_stock(source_asset_name='SPY', new_name='SYN', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', trend_list=[], adj_close_start=0, price_slope=0, close_scale=1.4):\n",
    "    \n",
    "    syn_df = get_data(asset_name=source_asset_name, directory=directory, start_date=start_date, stop_date=stop_date, force_yahoo=1)\n",
    "        \n",
    "    syn_length = len(syn_df)\n",
    "    \n",
    "    for i in range(syn_length):\n",
    "        mag = adj_close_start + (i * price_slope)\n",
    "        for trend in trend_list:\n",
    "            mag += (trend[0] * np.sin(6.28 * i / trend[1]))\n",
    "\n",
    "        syn_df['Adj Close'].loc[i] = mag\n",
    "        syn_df['Close'].loc[i] = mag * close_scale\n",
    "    \n",
    "        # Today's open is yesterday's close\n",
    "        if i > 0:\n",
    "            syn_df['Open'].loc[i] = syn_df['Close'].loc[i-1]\n",
    "        else:\n",
    "            syn_df['Open'].loc[i] = syn_df['Close'].loc[i]\n",
    "        \n",
    "        syn_df['Low'].loc[i] = syn_df['Open'].loc[i]\n",
    "        syn_df['High'].loc[i] = syn_df['Close'].loc[i]\n",
    "    \n",
    "    pathname = directory + '/' + new_name + '_' + start_date + '.csv'    \n",
    "    \n",
    "    syn_df.to_csv(pathname, index=False)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a Plotly interactive candelstick chart\n",
    "\n",
    "# path is directory to save image in\n",
    "# display_plot - 1=display plot in Notebook\n",
    "\n",
    "def plotly_candlestick(asset_df, asset_name, basefn = '', display_plot = 0, save_plot=1):\n",
    "\n",
    "    df = asset_df\n",
    "    \n",
    "    trace1 = {\n",
    "        'x': df.Date,\n",
    "        'open': df.Open,\n",
    "        'close': df.Close,\n",
    "        'high': df.High,\n",
    "        'low': df.Low,\n",
    "        'type': 'candlestick',\n",
    "        'name': asset_name,\n",
    "        'showlegend': True\n",
    "    }\n",
    "\n",
    "    data = [trace1]\n",
    "        \n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': 'Asset: ' + asset_name,\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(trace1, layout)\n",
    "    \n",
    "    if display_plot:        \n",
    "        fig.show()\n",
    "    \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'asset_candelstick.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_df(df):\n",
    "    # Add a feature called Prev Adj CLose\n",
    "    prev_adj_close = df['Adj Close'].shift(1).copy()\n",
    "    df['Prev Adj Close'] = prev_adj_close\n",
    "\n",
    "    # Take care of special case of very first previous Adj close - just make is the same as the Adj Close\n",
    "    df['Prev Adj Close'][0] = df['Prev Adj Close'][1]\n",
    "    \n",
    "    # Volume isn't used\n",
    "    df = df.drop(columns=['Volume'])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Cuda available:\",torch.cuda.is_available())\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will normalise each column of a dataframe to +/- 0.5\n",
    "\n",
    "def normalise_df(df):\n",
    "           \n",
    "    r = df.copy()\n",
    "    \n",
    "    # Get the min and max of all values in dataframe except 'Date' field\n",
    "    min_value = df.drop('Date',1).min().min()    \n",
    "    max_value = df.drop('Date',1).max().max()\n",
    "\n",
    "#    print(min_value)\n",
    "#    print(max_value)\n",
    "    \n",
    "#    r['Open'] = (r['Open'] - min_value) / (max_value - min_value) - 0.5\n",
    "#    r['Low'] = (r['Low'] - min_value) / (max_value - min_value) - 0.5\n",
    "#    r['High'] = (r['High'] - min_value) / (max_value - min_value) - 0.5\n",
    "#    r['Close'] = (r['Close'] - min_value) / (max_value - min_value) - 0.5\n",
    "#    r['Adj Close'] = (r['Adj Close'] - min_value) / (max_value - min_value) - 0.5\n",
    "#    r['Prev Adj Close'] = (r['Prev Adj Close'] - min_value) / (max_value - min_value) - 0.5    \n",
    "\n",
    "\n",
    "    r.loc[:, r.columns != 'Date'] = (((r.loc[:, r.columns != 'Date'] - min_value) / (max_value - min_value)) - 0.5)\n",
    "#    print(r.describe())\n",
    "    \n",
    "#    display(r)\n",
    "    scale = max_value - min_value\n",
    "    offset = min_value\n",
    "#    To un-normalise:\n",
    "    \n",
    "#    original_value = ((norm_price + 0.5) * scale) + offset\n",
    "#    scale = max_value - min_value\n",
    "#    offset = min_value\n",
    "\n",
    "#    print(\"values from df normalisation:\")\n",
    "#    print(\"Scale:\", scale)\n",
    "#    print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "#    display(r.head())\n",
    "#    display(df.head())\n",
    "    return r, scale, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalise_value(norm_value, scale, offset):\n",
    "    return (((norm_value + 0.5) * scale) + offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalise_df(df, scale, offset):\n",
    "    \n",
    "    result = df.copy()\n",
    "    \n",
    "    df = ((df + 0.5) * scale) + offset\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is for sequence to value learning - the label is a single value - tomorrow's Adj Close\n",
    "\n",
    "# Given one of the asset train, valid or test dataframes, this will make a new three dimensional list which has\n",
    "# dimensions (num_samples, window_size - tw, 6) which represents the effect of sliding a window of width window_size (tw)\n",
    "# down the dataframe from start to end and collecting the input features into a length 6 vector.  This will be our\n",
    "# input dataset to the LSTM.\n",
    "\n",
    "# In addition, also generate a list of (training) labels which is \"tomorrow's\" adjusted closing price - the thing \n",
    "# we are trying to predict. There is a single training label for each tw x 6 set of inputs.\n",
    "\n",
    "\n",
    "def create_input_sequences_out_val_from_df(input_data_df, tw):\n",
    "    in_seq = []\n",
    "    out_seq= []\n",
    "    L = len(input_data_df)\n",
    "\n",
    "    for i in range(L-tw):\n",
    "        seq = []\n",
    "        for j in range(tw):\n",
    "            features = [input_data_df['Open'].loc[i+j], input_data_df['Low'].loc[i+j], input_data_df['High'].loc[i+j], \n",
    "                        input_data_df['Close'].loc[i+j], \n",
    "                         input_data_df['Prev Adj Close'].loc[i+j], input_data_df['Adj Close'].loc[i+j]]\n",
    "            seq.append(features)\n",
    "\n",
    "        train_label = input_data_df['Adj Close'].loc[i+tw]\n",
    "        out_seq.append(train_label)\n",
    "        in_seq.append(seq)\n",
    "    return in_seq, out_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is for sequence to sequence learning - the label is a sequence up to and including tomorrows Adj Close\n",
    "\n",
    "def create_input_sequences_from_df(input_data_df, tw):\n",
    "    in_seq = []\n",
    "    out_seq= []\n",
    "    L = len(input_data_df)\n",
    "\n",
    "    for i in range(L-tw):\n",
    "        window_input_seq = []\n",
    "        window_output_seq = []\n",
    "        for j in range(tw):\n",
    "            features = [input_data_df['Open'].loc[i+j], input_data_df['Low'].loc[i+j], input_data_df['High'].loc[i+j], \n",
    "                        input_data_df['Close'].loc[i+j], \n",
    "                         input_data_df['Prev Adj Close'].loc[i+j], input_data_df['Adj Close'].loc[i+j]]\n",
    "            window_input_seq.append(features)\n",
    "            \n",
    "            window_output_seq.append(input_data_df['Adj Close'].loc[i+j+1])\n",
    "\n",
    "        out_seq.append(window_output_seq)\n",
    "        in_seq.append(window_input_seq)\n",
    "        \n",
    "#        print(\"seq:\", i)\n",
    "#        print(\"input:\\n\",window_input_seq)\n",
    "#        print(\"output:\\n\", window_output_seq)\n",
    "    return in_seq, out_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_color(colorscale, intermed):\n",
    "    \"\"\"\n",
    "    Plotly continuous colorscales assign colors to the range [0, 1]. This function computes the intermediate\n",
    "    color for any value in that range.\n",
    "\n",
    "    Plotly doesn't make the colorscales directly accessible in a common format.\n",
    "    Some are ready to use:\n",
    "    \n",
    "        colorscale = plotly.colors.PLOTLY_SCALES[\"Greens\"]\n",
    "\n",
    "    Others are just swatches that need to be constructed into a colorscale:\n",
    "\n",
    "        viridis_colors, scale = plotly.colors.convert_colors_to_same_type(plotly.colors.sequential.Viridis)\n",
    "        colorscale = plotly.colors.make_colorscale(viridis_colors, scale=scale)\n",
    "\n",
    "    :param colorscale: A plotly continuous colorscale defined with RGB string colors.\n",
    "    :param intermed: value in the range [0, 1]\n",
    "    :return: color in rgb string format\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if len(colorscale) < 1:\n",
    "        raise ValueError(\"colorscale must have at least one color\")\n",
    "\n",
    "    if intermed <= 0 or len(colorscale) == 1:\n",
    "        return colorscale[0][1]\n",
    "    if intermed >= 1:\n",
    "        return colorscale[-1][1]\n",
    "\n",
    "    for cutoff, color in colorscale:\n",
    "        if intermed >= cutoff:\n",
    "            low_cutoff, low_color = cutoff, color\n",
    "        else:\n",
    "            high_cutoff, high_color = cutoff, color\n",
    "            break\n",
    "\n",
    "    # noinspection PyUnboundLocalVariable\n",
    "    return plotly.colors.find_intermediate_color(\n",
    "        lowcolor=low_color, highcolor=high_color,\n",
    "        intermed=((intermed - low_cutoff) / (high_cutoff - low_cutoff)),\n",
    "        colortype=\"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_gradients(model, min_grads, max_grads, ave_grads, basefn='', display_plot=0, save_plot=1):\n",
    "    \n",
    "    scale = plotly.colors.PLOTLY_SCALES[\"Bluered\"]\n",
    "\n",
    "    print(\"Number of gradients randomly sampled:\", len(ave_grads))  \n",
    "    \n",
    "    \n",
    "    # Assemble the names of the layers\n",
    "    layers = []\n",
    "    for n,p in model.named_parameters():\n",
    "        if (p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "    \n",
    "    # Dong this the dumb way just now\n",
    "    max_array = np.zeros(len(layers))\n",
    "    min_array = np.ones(len(layers))\n",
    "    mean_array = np.zeros(len(layers))\n",
    "    \n",
    "    for i in range(len(min_grads)):\n",
    "        for j in range(len(layers)):\n",
    "            if max_grads[i][j].item() > max_array[j]:\n",
    "                max_array[j] = max_grads[i][j].item()\n",
    "            \n",
    "            mean_array[j] += ave_grads[i][j].item()\n",
    "            \n",
    "            if min_grads[i][j].item() < min_array[j]:\n",
    "                min_array[j] = min_grads[i][j].item()\n",
    "                   \n",
    "    mean_array /= len(min_grads)\n",
    "    \n",
    "    print(len(layers))\n",
    "    print(len(min_grads))\n",
    "    print(len(max_grads))\n",
    "    print(len(ave_grads))    \n",
    "\n",
    "    print(min_array)\n",
    "    print(mean_array)\n",
    "    print(max_array)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    df = pd.DataFrame(\n",
    "#        {'mean': [0.1,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "#        'max': [0.3,0.5,0.6,0.9,1.1,1.3,1.6],\n",
    "#        'min': [0.0,0.2,0.4,0.3,0.5,0.2,0.3],\n",
    "        {'mean': mean_array,\n",
    "        'max': max_array,\n",
    "        'min': min_array,         \n",
    "        'labels': layers})\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df['labels'], y=df['min'],\n",
    "        fill=None,\n",
    "        mode='lines',\n",
    "        line_color=get_continuous_color(scale, 0),\n",
    "        name=\"Min\",\n",
    "        line={'width': 4},\n",
    "        legendgroup=\"group1\"\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df['labels'],\n",
    "        y=df['mean'],\n",
    "        legendgroup=\"group2\",\n",
    "        fill='tonexty',\n",
    "        fillcolor=get_continuous_color(scale, 0.25).replace('rgb','rgba').replace(')',',0.5)'),\n",
    "        line_color=get_continuous_color(scale, 0.25),\n",
    "        mode='lines', name=\"Bottom Half\"))\n",
    "    fig.add_trace(go.Scatter(x=df['labels'], y=df['mean'],\n",
    "        fill=None,\n",
    "        mode='lines',\n",
    "        legendgroup=\"group1\",\n",
    "        line={'width': 4},\n",
    "        line_color=get_continuous_color(scale, 0.5),\n",
    "        name=\"Mean\"))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df['labels'],\n",
    "        y=df['max'],\n",
    "        legendgroup=\"group2\",\n",
    "        fill='tonexty',\n",
    "        fillcolor=get_continuous_color(scale, 0.75).replace('rgb','rgba').replace(')',',0.5)'),\n",
    "        line_color=get_continuous_color(scale, 0.75),\n",
    "        mode='lines',\n",
    "        name=\"Top Half\"))\n",
    "    fig.add_trace(go.Scatter(x=df['labels'], y=df['max'],\n",
    "        fill=None,\n",
    "        mode='lines',\n",
    "        legendgroup=\"group1\",\n",
    "        line={'width': 4},\n",
    "        line_color=get_continuous_color(scale, 1),\n",
    "        name=\"Max\"\n",
    "        ))\n",
    "\n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'gradients.html')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss and val_loss are lists containing the loss per step\n",
    "\n",
    "\n",
    "def plot_losses(train_loss, val_loss, title='Train / Validation Loss', basefn='', display_plot=0, save_plot=1):\n",
    "\n",
    "    iter = len(train_loss)\n",
    "    \n",
    "    # Plot the training / validation loss vs step\n",
    "    trace1 = {\n",
    "        'x': np.arange(0,len(train_loss)),\n",
    "        'y': np.array(train_loss),\n",
    "                  \n",
    "        'type': 'scatter',\n",
    "        'name': 'Training',\n",
    "        'showlegend': True\n",
    "    }\n",
    "\n",
    "    trace2 = {\n",
    "        'x': np.arange(0,len(val_loss)),\n",
    "        'y': np.array(val_loss),\n",
    "                  \n",
    "        'type': 'scatter',\n",
    "        'name': 'Validation',\n",
    "        'showlegend': True\n",
    "    }\n",
    "\n",
    "    data = [trace1, trace2]\n",
    "        \n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': title,\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data, layout)\n",
    "    \n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Train_losses.html')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns_hist(predicted_returns, actual_returns, title='Histogram: Predicted vs Actual Returns', basefn='', display_plot=0, save_plot=1):\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=predicted_returns, name='Predicted'))\n",
    "    fig.add_trace(go.Histogram(x=actual_returns, name='Actual'))\n",
    "    fig.update_layout(\n",
    "        title_text = title,\n",
    "        xaxis_title_text = 'Return',\n",
    "        yaxis_title_text = 'Count'\n",
    "    )\n",
    "    \n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "    \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Returns_hist.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden(h, basefn='', display_plot=0, save_plot=1):\n",
    "    \n",
    "    # h is a tuple containing 2 tensors (one for h, one for c). \n",
    "    # Each tensor is of dimension [num layers, batch size, hidden size]\n",
    "    # e.g. [3, 1, 128]\n",
    "        \n",
    "    titles = ['Hidden State', 'Cell State']\n",
    "    \n",
    "    x = np.arange(0, h[0].size()[2])\n",
    "        \n",
    "    # Loop through hidden and cell states\n",
    "    for state in range(2):\n",
    "        data = []\n",
    "        # Loop through each layer, add a trace for each\n",
    "        for layer in range(h[state].size()[0]):\n",
    "            h_tensor = h[state][layer][0].cpu()\n",
    "            trace = {\n",
    "                'x': x,                \n",
    "                'y': np.array(h_tensor.detach().numpy()),\n",
    "                  \n",
    "                'type': 'scatter',\n",
    "                'name': 'Layer '+str(layer),\n",
    "                'showlegend': True\n",
    "                }\n",
    "            data.append(trace)\n",
    "        \n",
    "        layout = go.Layout({\n",
    "            'title': {\n",
    "                'text': titles[state],\n",
    "                'font': {\n",
    "                    'size': 20\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        fig = go.Figure(data, layout)\n",
    "        \n",
    "        if display_plot:\n",
    "            fig.show()\n",
    "        \n",
    "        if save_plot:\n",
    "            fig.write_html(basefn + 'Train_hidden_layer_snapshot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lstm_output(lstm_output, basefn='', display_plot=0, save_plot=1):\n",
    "    \n",
    "    # lstm_output is a tuple containing [sequence length] number of tensors.\n",
    "    # Each tensor is the lstm output with dimensions [batch size, hidden size]\n",
    "    # e.g. [1, 128]\n",
    "    \n",
    "    x = np.arange(0, lstm_output.size()[1])\n",
    "        \n",
    "    data = []\n",
    "    \n",
    "    # Loop through each layer, add a trace for each\n",
    "    for i in range(len(lstm_output)):\n",
    "        lstm_tensor = lstm_output[i].cpu()\n",
    "        trace = {\n",
    "            'x': x,\n",
    "            'y': np.array(lstm_tensor.detach().numpy()),\n",
    "                  \n",
    "            'type': 'scatter',\n",
    "            'name': 'Cell '+str(i),\n",
    "            'showlegend': True\n",
    "            }\n",
    "        data.append(trace)\n",
    "        \n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': 'LSTM Output',\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data, layout)\n",
    "    \n",
    "    if display_plot:\n",
    "        fig.show()   \n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Train_LSTM_output_snapshot.html')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(model, basefn='', display_plot=0, save_plot=1):\n",
    "    \n",
    "    # Plot the linear weights - just a simple line chart\n",
    "    linear_tensor = model.linear.weight[0].cpu()\n",
    "    x = np.arange(0, len(model.linear.weight[0]))\n",
    "    y = np.array(linear_tensor.detach().numpy())\n",
    "  \n",
    "    # Plot the training / validation loss vs step\n",
    "    trace1 = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "                  \n",
    "        'type': 'scatter',\n",
    "        'name': 'Linear Weights',\n",
    "        'showlegend': True\n",
    "    }\n",
    "\n",
    "    data = [trace1]\n",
    "        \n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': 'Linear Weights',\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data, layout)\n",
    "    \n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Train_linear_weights.html')\n",
    "    \n",
    "    ave_weights = []\n",
    "    max_weights = []\n",
    "    layers = []\n",
    "    for n,p in model.named_parameters():\n",
    "        if (p.requires_grad) and (\"bias\" not in n) and (\"linear\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_weights.append(p.abs().mean().item())\n",
    "            max_weights.append(p.abs().max().item())\n",
    "\n",
    "    trace1 = {\n",
    "        'x' : layers,\n",
    "        'y' : ave_weights,\n",
    "        'type' : 'bar'\n",
    "    }    \n",
    "\n",
    "    data = [trace1]\n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': 'Average of LSTM Weights',\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data, layout)\n",
    "\n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Train_ave_lstm_weights.html')\n",
    "    \n",
    "    \n",
    "    trace1 = {\n",
    "        'x' : layers,\n",
    "        'y' : max_weights,\n",
    "        'type' : 'bar'\n",
    "    }    \n",
    "\n",
    "    data = [trace1]\n",
    "    layout = go.Layout({\n",
    "        'title': {\n",
    "            'text': 'MAX of LSTM Weights',\n",
    "            'font': {\n",
    "                'size': 20\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data, layout)\n",
    "    \n",
    "    if display_plot:\n",
    "        fig.show()\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.write_html(basefn + 'Train_max_lstm_weights.html')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_plot(layer_names, grads, color, title, basefn='', display_plot=0, save_plot=1):\n",
    "       \n",
    "    for i in range(len(grads)):\n",
    "        plt.plot(grads[i], alpha=0.3, color=color)\n",
    "    \n",
    "    plt.hlines(0, 0, len(grads)+1, linewidth=1, color=\"k\")\n",
    "    plt.xticks(range(0,len(grads[0]),1), layer_names, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(grads[0]))\n",
    "    \n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "           \n",
    "    # This has to go before the plt.show() otherwise it's blank\n",
    "    if save_plot:\n",
    "        plt.savefig(basefn + title + '.png', bbox_inches='tight')\n",
    "\n",
    "    if display_plot:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients(model, ave_grads, max_grads, basefn='', display_plot=0, save_plot=1):\n",
    "\n",
    "    print(\"Number of gradients randomly sampled:\", len(ave_grads))    \n",
    "    \n",
    "    # Assemble the names of the layers\n",
    "    layers = []\n",
    "    for n,p in model.named_parameters():\n",
    "        if (p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "\n",
    "    gradient_plot(layers, ave_grads, 'b', 'Gradient Flow - All Layers - Average', basefn=basefn, display_plot=display_plot, save_plot=save_plot)\n",
    "    gradient_plot(layers, max_grads, 'r', 'Gradient Flow - All Layers - Max', basefn=basefn, display_plot=display_plot, save_plot=save_plot)\n",
    "        \n",
    "    ave_grads_lstm = [sublist[:len(layers)-1] for sublist in ave_grads]\n",
    "    max_grads_lstm = [sublist[:len(layers)-1] for sublist in max_grads]\n",
    "    layers_lstm = layers[0:len(layers)-1]\n",
    "    \n",
    "    gradient_plot(layers_lstm, ave_grads_lstm, 'b', 'Gradient Flow - LSTM Layers - Average', basefn=basefn, display_plot=display_plot, save_plot=save_plot)\n",
    "    gradient_plot(layers_lstm, max_grads_lstm, 'r', 'Gradient Flow - LSTM Layers - Max', basefn=basefn, display_plot=display_plot, save_plot=save_plot)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeilLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 6, window_size = 11, arch = 'LH', hidden_size = 64, output_size = 1, num_layers = 3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store some stuff we need to know in the forward pass\n",
    "        self.n_layers = num_layers        \n",
    "        self.hidden_layer_size = hidden_size\n",
    "        self.arch = arch\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=False, bias=True, dropout=dropout, bidirectional=False)\n",
    "                \n",
    "        # Define the linear layer\n",
    "        # Perhaps should perform forward on the lstm to explicitly get the size of the \n",
    "        # tensor for sequence to one vs sequence to sequence versions.\n",
    "        \n",
    "        # The size of the linear layer for LH or AH model\n",
    "        if arch == 'LH':\n",
    "            self.linear = nn.Linear(hidden_size, output_size, bias = True)\n",
    "        elif arch == 'AH':\n",
    "            self.linear = nn.Linear(hidden_size * window_size, output_size, bias = True)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        \n",
    "        # NEED TO FIX THIS STUPID HARD CODED INIT\n",
    "        if num_layers >= 1:\n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_hh_l0)\n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_ih_l0)\n",
    "            \n",
    "        if num_layers >= 2:\n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_hh_l1)        \n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_ih_l1)                    \n",
    "        \n",
    "        if num_layers >= 3:\n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_hh_l2)                        \n",
    "            torch.nn.init.xavier_uniform_(self.lstm.weight_ih_l2)                        \n",
    "        \n",
    "        \n",
    "    # Forward pass\n",
    "    # input_seq shape is: (sequence length, num input features) - there's no batch dimension\n",
    "    def forward(self, input_seq, hidden, verbose=0):\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\\nlstm input:\\n\", input_seq)\n",
    "            print(\"\\n\\nlstm input view:\\n\", input_seq.view( len(input_seq) , 1 , -1))\n",
    "            print(\"\\n\\nlstm hidden input:\\n\", hidden)\n",
    "            \n",
    "        lstm_out, hidden = self.lstm( input_seq.view( len(input_seq) , 1 , -1), hidden)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\\nlstm_output:\\n\", lstm_out)\n",
    "            print(\"\\n\\nhidden output:\\n\", hidden)\n",
    "                    \n",
    "        # Feed forward the outputs of the lstm into the linear layer       \n",
    "        # lstm_out dims are (seq_len, batch, hidden dim)\n",
    "        # This is converted to (seq_len, hidden_dim) before input to the linear layer        \n",
    "        # When T=22 and hidden_layer_size=64, the shape of the lstm_out tensor is [22,1,64]    \n",
    "        # This view reshapes it to [22, 64]\n",
    "        lstm_out_view = lstm_out.view(len(input_seq), -1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\\nlstm_out_view:\\n\", lstm_out_view)\n",
    "                                       \n",
    "        if verbose:\n",
    "            print(\"\\n\\nLinear weights:\\n\", self.linear.weight)\n",
    "            print(\"\\n\\ninput to fc:\\n\", lstm_out_view[-1])\n",
    "        \n",
    "        # LH - Just present the last hidden state to the linear network            \n",
    "        if self.arch == 'LH':\n",
    "            predictions = self.linear(lstm_out_view[-1])\n",
    "        # AH - flatten and present all to linear network\n",
    "        elif self.arch == 'AH':\n",
    "            predictions = self.linear(torch.flatten(lstm_out_view))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\\npredictions:\\n\",predictions)\n",
    "        \n",
    "        # predictions has dimension: (1, seq_len)        \n",
    "        return predictions, hidden, lstm_out_view\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_().to(device),\n",
    "                 weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_().to(device))\n",
    "        return(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the given LSTM model. Also collects validation data (predicting the next day) and collects stats\n",
    "# for both.  These are used to observe training vs validation but also to construct the first allocation policy.\n",
    "\n",
    "# Q version has removed inner loop instrumentation and made other spped changes\n",
    "# This version runs about 20% faster than the non-Q version\n",
    "\n",
    "def train_modelQ(model, train_seq, train_labels, scale=1, offset=1, train_start=0, train_length=1, epochs = 20, \n",
    "                 iter_per_seq=1, max_iter=1600, lr = 0.001, dr=0.999, wd=0.0, verbosity=0, basefn='',\n",
    "                 percentiles=[0,10,20,30,40,50,60], trading_validation_period=100, validation_trials=10, \n",
    "                 valid_seq=[], valid_labels=[], plot=0):\n",
    "    \n",
    "    input_size = 6\n",
    "    \n",
    "    \n",
    "    # if plot level is 4, plot graphs on screen for every inner loop training run\n",
    "    if plot == 4:\n",
    "        dump_log_interval = 1\n",
    "    else:\n",
    "        dump_log_interval = 10 # otherwise do it every 10\n",
    "        \n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = dr)\n",
    "    \n",
    "    batch_size = 1  # batch mode isn't used actually but including for future possible use\n",
    "    \n",
    "    with open(basefn + 'train.csv', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Epc','AvTrLoss','AvTrMSE','AvValMSE','MdnTrMSE','MdnValMSE','ValMDA','ValMAPE','ValMAE','PXcorr','RXcorr',\n",
    "                     'EpcSecs','TtlMins'])\n",
    "        \n",
    "    with open(basefn + 'trade_validation.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Epoch','MDA','MSE','MAE','MAPE','X-Correl','Num-trades','AveRt%','MdnRT%','MinRt%','MaxRt%',\n",
    "                    'RangeRt%','StdDevRt%'])\n",
    "            \n",
    "    # Outputs a summary line for each Epoch\n",
    "    if verbosity == 1:\n",
    "        print('Epc  AvTrLoss  AvTrMSE   AvValMSE  MdnTrMSE  MdnValMSE  ValMDA    ValMAPE  ValMAE    PXcorr  RXcorr Secs    TTlMins')\n",
    "\n",
    "    # Outputs a summary line for each sequence in each epoch\n",
    "    if verbosity == 2:\n",
    "        print('EPc  Seq#   TrLoss    TrMSE     PredR   ActRt   ValLoss   VAdjC    VPred   VAct      VPredRt   VActRt   RunMDA')            \n",
    "                            \n",
    "    \n",
    "#    train_losses = []\n",
    "#    val_adj_close_price = []\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    min_grads = []\n",
    "    \n",
    "    # If the number of epochs is high, just sample the gradients stochastically to avoid retaining too much\n",
    "    # data.\n",
    "    if (epochs * train_length) > 1000:\n",
    "        sample_grads = 1000.0 / (epochs * train_length)\n",
    "    else:\n",
    "        sample_grads = 1.0\n",
    "\n",
    "    start_time = time.time()       \n",
    "    \n",
    "    train_seq_cuda = train_seq.to(device)\n",
    "    train_labels_cuda = train_labels.to(device)\n",
    "\n",
    "    # How many times through the entire training set\n",
    "    for i in range(epochs):\n",
    "        train_losses = []\n",
    "        val_adj_close_price = []        \n",
    "        train_mses = []\n",
    "        val_mses = []\n",
    "        val_pred_rt = []\n",
    "        val_act_rt = []\n",
    "        val_pred_price = []\n",
    "        val_act_price = []        \n",
    "        running_mda = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Each sample in the training set\n",
    "        for j in range(train_start, train_start + train_length):      \n",
    "                        \n",
    "            inputs = train_seq_cuda[j]\n",
    "            labels = train_labels_cuda[j]\n",
    "            \n",
    "            # How many times we train on each sample as we go\n",
    "            for iter in range(iter_per_seq):\n",
    "                # Initialise hidden states before every training event\n",
    "                h = model.init_hidden(batch_size)  \n",
    "                                    \n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                y_pred, h, lstm_out = model(inputs, h)\n",
    "            \n",
    "                # y_pred and labels are both size T - sequence to sequence loss function\n",
    "                train_loss = loss_function(y_pred, labels)\n",
    "            \n",
    "                # Compute gradients and update the model\n",
    "                train_loss.backward()\n",
    "                                \n",
    "                optimizer.step()\n",
    "                \n",
    "            # For speed, we only do outer loop validation at the same time we do trading validation (the stats are\n",
    "            # needed to compute the initial allocation policy)\n",
    "            \n",
    "            if i % trading_validation_period == 0:\n",
    "                # Sample stochastically the last set of gradients after the inner loop for plotting later\n",
    "                if plot and (random.random() < sample_grads):\n",
    "                    ave_grad=[]\n",
    "                    max_grad=[]\n",
    "                    min_grad=[]\n",
    "                    for n,p in model.named_parameters():\n",
    "                        if (p.requires_grad) and (\"bias\" not in n):\n",
    "                            ave_grad.append(p.grad.abs().mean())\n",
    "                            max_grad.append(p.grad.abs().max())\n",
    "                            min_grad.append(p.grad.abs().min())\n",
    "                    ave_grads.append(ave_grad)\n",
    "                    max_grads.append(max_grad)\n",
    "                    min_grads.append(min_grad)            \n",
    "            \n",
    "                # Capture the (snapshot) metrics from the last inner loop cycle\n",
    "                training_loss = train_loss.item()  # This is the sequence to sequence training loss                        \n",
    "                train_adj_close = inputs[-1][-1].item()   # Adj close at t=0\n",
    "                train_pred = y_pred[-1].item()  # Next day predicted Adj close\n",
    "                train_truth = labels[-1].item()  # Next day actual Adj close\n",
    "                train_mse = (train_pred - train_truth)**2   # MSE of just the predicted value vs truth\n",
    "                train_predicted_return = (denormalise_value(train_pred, scale, offset) / denormalise_value(train_adj_close, scale, offset)) - 1            \n",
    "                train_actual_return = (denormalise_value(train_truth, scale, offset) / denormalise_value(train_adj_close, scale, offset)) - 1\n",
    "\n",
    "                # Outer loop validation every trading_validation_period epochs\n",
    "                model.eval()\n",
    "            \n",
    "                inputs = train_seq[j+1].to(device)\n",
    "                labels = train_labels[j+1].to(device)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    h = model.init_hidden(batch_size)\n",
    "        \n",
    "                    y_pred, h, lstm_out = model(inputs, h)\n",
    "                    \n",
    "                    valid_adj_close = inputs[-1][-1].item()                \n",
    "                    valid_pred = y_pred[-1].item()\n",
    "                    valid_truth = labels[-1].item()\n",
    "                    valid_loss = (valid_pred - valid_truth)**2  # Just the mse of the valid predicted value vs truth\n",
    "            \n",
    "                    valid_predicted_return = (denormalise_value(valid_pred, scale, offset) / denormalise_value(valid_adj_close, scale, offset)) - 1\n",
    "                    valid_actual_return = (denormalise_value(valid_truth, scale, offset) / denormalise_value(valid_adj_close, scale, offset)) - 1\n",
    "\n",
    "                    if (valid_predicted_return * valid_actual_return >= 0):\n",
    "                        running_mda += 1                \n",
    "                \n",
    "                    # If enabled, print results line at the end of inner loop training on each sequence\n",
    "                    if (verbosity==2):\n",
    "                        print('{:4}'.format(i),\n",
    "                            '{:4}'.format(j), \n",
    "                          '  {:1.1e}'.format(training_loss),\n",
    "                          '  {:1.1e}'.format(train_mse),\n",
    "                          ' {: 2.3f}'.format(100.0*train_predicted_return), \n",
    "                          ' {: 2.3f}'.format(100.0*train_actual_return),\n",
    "                          '  {:1.1e}'.format(valid_loss), \n",
    "                          ' {: 1.4f}'.format(valid_adj_close), \n",
    "                          ' {: 1.4f}'.format(valid_pred), \n",
    "                          ' {: 1.4f}'.format(valid_truth),\n",
    "                          ' {: 2.4f}'.format(100.0*valid_predicted_return), \n",
    "                          ' {: 2.4f}'.format(100.0*valid_actual_return),\n",
    "                          '  {:2.2f}'.format(100.0*running_mda/(j+1)))\n",
    "                    \n",
    "                    # Keep lists of the results of train / validation for each sequence for passing back up the stack\n",
    "                    train_losses.append(training_loss)\n",
    "                    train_mses.append(train_mse)\n",
    "                    val_mses.append(valid_loss)\n",
    "                    val_pred_rt.append(valid_predicted_return)\n",
    "                    val_act_rt.append(valid_actual_return)\n",
    "                    val_pred_price.append(valid_pred)\n",
    "                    val_act_price.append(valid_truth)\n",
    "                    val_adj_close_price.append(valid_adj_close)\n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "            # End of outer loop\n",
    "            \n",
    "        my_lr_scheduler.step()            \n",
    "           \n",
    "        elapsed_secs = time.time()-start_time\n",
    "        print(i, time.time()-epoch_start_time, elapsed_secs, optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if i % trading_validation_period == 0:\n",
    "            # We report stats on the last portion of the training\n",
    "            stats_len = int(0.5 * len(train_losses))\n",
    "            \n",
    "            metrics_dict = compute_metrics(np.array(val_pred_rt[-stats_len:]),\n",
    "                                   np.array(val_act_rt[-stats_len:]),\n",
    "                                   np.array(val_pred_price[-stats_len:]),\n",
    "                                   np.array(val_act_price[-stats_len:]))\n",
    "\n",
    "            # This is the results we'll pass back up the stack for the entire training\n",
    "            result_dict = {'trStats_N':stats_len, 'AvTrLoss':np.mean(train_losses[-stats_len:]),\n",
    "                   'trAvTrMSE':np.mean(train_mses[-stats_len:]),\n",
    "                   'trAvValMSE':np.mean(val_mses[-stats_len:]),\n",
    "                   'trMdnTrMSE':np.median(train_mses[-stats_len:]),\n",
    "                   'trMdnValMSE':np.median(val_mses[-stats_len:]),\n",
    "                   'train_mses':train_mses, 'valid_mses':val_mses, 'val_pred_rt':val_pred_rt, 'val_act_rt':val_act_rt, \n",
    "                   'val_pred_price':val_pred_price, 'val_act_price':val_act_price, \n",
    "                   'val_adj_close_price': val_adj_close_price,\n",
    "                    'trMDA': metrics_dict['MDA'],\n",
    "                    'trMAPE': metrics_dict['MAPE'],\n",
    "                    'trMAE': metrics_dict['MAE'],\n",
    "                    'trPXCORR': metrics_dict['PXCORR'],\n",
    "                    'trRXCORR': metrics_dict['RXCORR'],\n",
    "                    'epoch_train_time':time.time()-epoch_start_time,\n",
    "                   'train_time':elapsed_secs}\n",
    "        \n",
    "            # Output summary stats at end of outer loop training  \n",
    "            if (verbosity == 1):\n",
    "                print('{:4}'.format(i),'{:4.2e}'.format(result_dict['AvTrLoss']),\n",
    "                          ' {:1.2e}'.format(result_dict['trAvTrMSE']),\n",
    "                          ' {:4.2e}'.format(result_dict['trAvValMSE']), \n",
    "                          ' {:1.2e}'.format(result_dict['trMdnTrMSE']),\n",
    "                          ' {:4.2e}'.format(result_dict['trMdnValMSE']),                  \n",
    "                          '  {:2.2f}'.format(result_dict['trMDA']), \n",
    "                          '    {:2.2f}'.format(result_dict['trMAPE']), \n",
    "                          '    {:2.2e}'.format(result_dict['trMAE']),\n",
    "                          '{: 1.2f}'.format(result_dict['trPXCORR']), \n",
    "                          '  {: 1.2f}'.format(result_dict['trRXCORR']),      \n",
    "                          '  {:2.2f}'.format(result_dict['epoch_train_time']),                  \n",
    "                          '  {:3.1f}'.format(result_dict['train_time']/60.0))\n",
    "            \n",
    "            with open(basefn + 'train.csv', 'a', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow([i, result_dict['AvTrLoss'],\n",
    "                        result_dict['trAvTrMSE'],\n",
    "                        result_dict['trAvValMSE'],\n",
    "                        result_dict['trMdnTrMSE'],\n",
    "                        result_dict['trMdnValMSE'],\n",
    "                        result_dict['trMDA'],\n",
    "                        result_dict['trMAPE'],\n",
    "                        result_dict['trMAE'],\n",
    "                        result_dict['trPXCORR'],\n",
    "                        result_dict['trRXCORR'],\n",
    "                        result_dict['epoch_train_time'],\n",
    "                        result_dict['train_time']/60.0,\n",
    "                        ])\n",
    "        \n",
    "        # Periodically perform a trading validation \n",
    "        if i % trading_validation_period == 0:\n",
    "            \n",
    "            # Make a results directory. If the 'directory' doesn't exist, create it\n",
    "            if os.path.isdir(basefn + '/saved_models') == False:\n",
    "                os.mkdir(basefn + '/saved_models')\n",
    "            \n",
    "            D,Q,E,A,C = generate_initial_allocation_policy(result_dict, percentiles=percentiles, scale=scale, offset=offset, basefn=basefn + 'saved_models/'+str(i), verbosity=0)\n",
    "            \n",
    "            # Save both the model and the initial allocation policy so we can recreate for validation / test after training\n",
    "            # (pick the best model out of a complete training run after analysing the data)\n",
    "                        \n",
    "            torch.save(model,basefn + '/saved_models/epoch' + str(i) + '.mdl')\n",
    "            \n",
    "            with open(basefn + '/saved_models/DQEAC_dict' + str(i) + '.pkl','wb') as fp:\n",
    "                pickle.dump({'D':D, 'Q':Q, 'E':E, 'A':A, 'C':C}, fp, protocol=pickle.HIGHEST_PROTOCOL)            \n",
    "\n",
    "            # Note - prior to version 15, I wasn't deep copying the D,Q,E,A which means they would have been being updated\n",
    "            # by trading validation.\n",
    "            trial_results = []\n",
    "            for trial in range(validation_trials):\n",
    "                # For each trial make a deep copy of things that get modified by the validation process. Make sure that \n",
    "                # the model being trained and the allocation policy components are not being modified and we always start\n",
    "                # validation from the same starting point.\n",
    "                validation_model = copy.deepcopy(model)\n",
    "                copy_D = copy.deepcopy(D)\n",
    "                copy_Q = copy.deepcopy(Q)\n",
    "                copy_E = copy.deepcopy(E)\n",
    "                copy_A = copy.deepcopy(A)\n",
    "                \n",
    "                validation_model, copy_D, copy_Q, copy_E, copy_A, journal, test_result_dict = test_trading_system(validation_model, valid_seq, valid_labels, copy_D, copy_Q, copy_E, copy_A, percentiles, basefn=basefn+'/saved_models/'+str(i)+'_'+str(trial), scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = 0.001, dr = 1, verbosity=1)\n",
    "                \n",
    "                trial_results.append(test_result_dict)                \n",
    "\n",
    "            \n",
    "            mean_trial_MDA = np.mean([v['MDA'] for v in trial_results])\n",
    "            mean_trial_MSE = np.mean([v['MSE'] for v in trial_results])\n",
    "            mean_trial_MAE = np.mean([v['MAE'] for v in trial_results])\n",
    "            mean_trial_MAPE = np.mean([v['MAPE'] for v in trial_results])\n",
    "            mean_trial_RXCORR = np.mean([v['RXCORR'] for v in trial_results])\n",
    "            mean_trial_num_trades = np.mean([v['num_trades'] for v in trial_results])\n",
    "            trial_returns = [v['trade_return'] for v in trial_results]\n",
    "            mean_trial_trade_return = np.mean(trial_returns)\n",
    "            median_trial_trade_return = np.median(trial_returns)\n",
    "            min_trial_trade_return = np.min(trial_returns)\n",
    "            max_trial_trade_return = np.max(trial_returns)\n",
    "            range_trial_trade_return = max_trial_trade_return - min_trial_trade_return\n",
    "            std_trial_trade_return = np.std(trial_returns)\n",
    "            \n",
    "            print('Trading Validation - ',\n",
    "                'N:', validation_trials,\n",
    "                'MDA:{:2.2f}'.format(mean_trial_MDA),\n",
    "                '  Returns Correl:{:2.2f}'.format(mean_trial_RXCORR),\n",
    "                '  Num Trades:{:4}'.format(mean_trial_num_trades),\n",
    "                '  Return:{:2.2f}%'.format(mean_trial_trade_return),\n",
    "                '  StdDev:{:2.2f}%'.format(std_trial_trade_return))\n",
    "            \n",
    "            with open(basefn + 'trade_validation.csv', 'a', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow([i,mean_trial_MDA, mean_trial_MSE,\n",
    "                            mean_trial_MAE, mean_trial_MAPE,\n",
    "                            mean_trial_RXCORR, mean_trial_num_trades, \n",
    "                            mean_trial_trade_return, median_trial_trade_return,\n",
    "                            min_trial_trade_return, max_trial_trade_return,\n",
    "                            range_trial_trade_return, std_trial_trade_return])\n",
    "\n",
    "    if plot>=1:\n",
    "        plot_gradients(model, ave_grads, max_grads, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plotly_gradients(model, min_grads, max_grads, ave_grads, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_hidden(h, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_lstm_output(lstm_out, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_weights(model, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_losses(train_mses, val_mses, title='Model Training: Training Loss vs Validation Loss', basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_pred_rt, val_act_rt, title='Model Training: Predicted vs Actual Returns', basefn=basefn+'both_', display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_pred_rt, val_pred_rt, title='Model Training: Predicted Returns', basefn=basefn+'pred_', display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_act_rt, val_act_rt, title='Model Training: Actual Returns', basefn=basefn+'act_', display_plot=plot, save_plot=1)    \n",
    "            \n",
    "    return model, result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a model instance and trains it on the training set\n",
    "\n",
    "def create_train_model(df, resume_dir='', resume_epoch=-1, basefn='', scale=1, offset=1, arch='LH', window_size = 22, epochs=20, iter_per_seq=1, max_iter=1600, num_layers=2, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0.0, percentiles=[0,10,20,30,40,50,60], trading_validation_period = 100, validation_trials=10, valid_seq=[], valid_labels=[], verbosity=0, plot=0):\n",
    "\n",
    "    # Generate training set\n",
    "    train_seq, train_labels = create_input_sequences_from_df(df, window_size)\n",
    "\n",
    "    # Convert our list of lists to Tensors\n",
    "    train_seq = torch.FloatTensor(train_seq)\n",
    "    train_labels = torch.FloatTensor(train_labels)\n",
    "       \n",
    "    if verbosity>=1:\n",
    "        print(\"Building LSTM Model with the following parameters:\")\n",
    "        print(\"Architecture:\", arch)\n",
    "        print(\"Num layers:\", num_layers)\n",
    "        print(\"Hidden size:\", hidden_size)\n",
    "        print(\"Dropout:\", dropout)\n",
    "        print(\"Learning Rate:\", lr)\n",
    "        print(\"LR Decay Rate:\", dr)\n",
    "        print(\"Weight Decay:\", wd)\n",
    "        print(\"Window size:\", window_size)\n",
    "        print(\"Num Epochs:\", epochs)                \n",
    "        print(\"Iterations per seq:\", iter_per_seq)\n",
    "        print(\"Start training...\")\n",
    "\n",
    "           \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # If we are starting from scratch, create a brand new model\n",
    "    if resume_dir=='':\n",
    "        model = NeilLSTM(input_size=6, window_size=window_size, arch=arch, hidden_size=hidden_size, output_size=window_size, num_layers=num_layers, dropout=dropout)\n",
    "        model_fn = 'NONE'\n",
    "    else: # Otherwise, load it from disk\n",
    "        # Special case, load the last model in directory\n",
    "        if resume_epoch == -1:\n",
    "            files = glob.glob(resume_dir + '/saved_models/*.mdl')\n",
    "            model_fn = max(files, key=os.path.getctime)\n",
    "        else:\n",
    "            model_fn = resume_dir + '/saved_models/epoch' + str(resume_epoch) + '.mdl'\n",
    "                        \n",
    "        model = torch.load(model_fn)\n",
    "        if verbosity >= 1:\n",
    "            print(\"Loaded existing model:\", model_fn)\n",
    "\n",
    "# Dump the model configuration and results to log file\n",
    "    with open(basefn + 'train.csv', 'w', newline='') as myfile:\n",
    "        \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Norm Scale:', scale])\n",
    "        wr.writerow(['Norm Offset:', offset])\n",
    "        wr.writerow(['Arch:', arch])\n",
    "        wr.writerow(['Window Size:', window_size])\n",
    "        wr.writerow(['Hidden Size:', hidden_size])\n",
    "        wr.writerow(['Num Layers:', num_layers])        \n",
    "        wr.writerow(['Dropout:', dropout])                \n",
    "        wr.writerow(['Learning Rate:', lr]) \n",
    "        wr.writerow(['LR Decay Rate:', dr])\n",
    "        wr.writerow(['Weight Decay:', wd])        \n",
    "        wr.writerow(['Num Epochs:', epochs])\n",
    "        wr.writerow(['Iterations Per Sequence:', iter_per_seq])\n",
    "        wr.writerow(['Start Epoch:', resume_epoch])\n",
    "        wr.writerow(['Resume-from model path:', model_fn])\n",
    "        wr.writerow(['Training Start Date:', df['Date'].iloc[0]])\n",
    "        wr.writerow(['Training End Date:', df['Date'].iloc[-1]])                    \n",
    "                \n",
    "    model.to(device)\n",
    "\n",
    "    # Stop one short of the end as we use the t+1 for validation during training\n",
    "    model, result_dict = train_modelQ(model, train_seq, train_labels, scale=scale, offset=offset, train_start=0, \n",
    "                                     train_length=len(train_seq)-1, epochs=epochs, iter_per_seq=iter_per_seq, \n",
    "                                     max_iter=max_iter, lr=lr, dr=dr, wd=wd, verbosity=verbosity, basefn=basefn, \n",
    "                                     plot=plot, trading_validation_period=trading_validation_period,\n",
    "                                     validation_trials=validation_trials,\n",
    "                                     percentiles=percentiles, valid_seq=valid_seq, valid_labels=valid_labels)\n",
    "        \n",
    "    # Save the model with the unique filename\n",
    "    torch.save(model,basefn + '_train.mdl')\n",
    "    \n",
    "    return model, result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_xcorrel(a, b):\n",
    "    c = np.corrcoef(a, b)\n",
    "\n",
    "    return c[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the interim metrics per the paper table 8 - MDA, MAPE, MAE, MSE\n",
    "# Also added cross-correlation\n",
    "\n",
    "# Pass in lists of next day predicted and actual returns, predicted and actual prices\n",
    "def compute_metrics(pred_rt, act_rt, pred_price, act_price):\n",
    "    \n",
    "    len_data = len(pred_rt)\n",
    "\n",
    "    product = pred_rt * act_rt\n",
    "    abs_price_diff = np.abs(act_price - pred_price)\n",
    "    \n",
    "    MDA = 100 * len(np.where(product > 0)[0]) / len_data\n",
    "    MAPE = 100 * np.sum(abs_price_diff / np.abs(act_price)) / len_data\n",
    "    MAE = np.sum(abs_price_diff) / len_data\n",
    "    MSE = np.sum(abs_price_diff ** 2) / len_data\n",
    "    \n",
    "    price_xcorrel = compute_norm_xcorrel(pred_price, act_price)\n",
    "    returns_xcorrel = compute_norm_xcorrel(pred_rt, act_rt)\n",
    "    \n",
    "    metrics_dict = {'N':len_data, 'MDA':MDA, 'MAPE': MAPE, 'MAE': MAE, 'MSE':MSE, 'PXCORR':price_xcorrel, \n",
    "                    'RXCORR':returns_xcorrel}\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this routine whenever a new predicted return is generated (each new day)\n",
    "\n",
    "# inputs:\n",
    "# D - a list of all the predicted returns, by trading day, to date\n",
    "# Q - a list of the bin cut-offs\n",
    "# new_pred_return - the new predicted return from the model\n",
    "# percentiles - a list of percentiles used to determine the bins (e.g. [0,10,20,30,40,50,60])\n",
    "\n",
    "# returns:\n",
    "# D - Updated\n",
    "# Q - updated\n",
    "\n",
    "def update_DQ(D, Q, new_pred_return, percentiles):\n",
    "\n",
    "    # Append the new predicted return to the existing list\n",
    "    D = np.append(D,new_pred_return)\n",
    "    \n",
    "    # If the predicted return is negative, it won't affect the binning calculation\n",
    "#    if new_pred_return <= 0:\n",
    "#        return D, Q\n",
    "    \n",
    "    # Percentile the positive returns\n",
    "    returns = np.array(D)\n",
    "#    Q = np.percentile(returns[returns >= 0], percentiles)\n",
    "    Q = np.percentile(abs(returns), percentiles)\n",
    "    \n",
    "    # Q now contains the cutoff values for each bin.  Each bin will contain approximately 10% of the positive\n",
    "    # returns except the last one which is an amalgamation of 4 bins so it will contain approx 40%\n",
    "        \n",
    "    # return updated D and Q\n",
    "    return D, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this routine whenever a new sell is executed (and therefore we have a new expected value for a bin)\n",
    "\n",
    "def update_EA(E, A, profit, purchase_bin):\n",
    "\n",
    "    # Append the new profit amount to the correct bin in E        \n",
    "    E[purchase_bin] = E[purchase_bin] + profit\n",
    "    \n",
    "    # Update A - bins with positive total profits are set to 1 (a buy signal)\n",
    "    # bins with negative profits are set to 0 - a do nothing signal\n",
    "    # bin 0 is the unique, sell signal\n",
    "    A = (E > 0).astype(int)    \n",
    "    \n",
    "    return E, A\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function combines model training (updates), predicting next day return, trading and updating the trading\n",
    "# policy each day.\n",
    "# Much of the code is a repeat / combo of the above two seperate routines of training the model and \n",
    "# genertaing the policy. This is different as we are now \"live\" and updating the model / policy only on\n",
    "# previously unseen data.\n",
    "# Also, although the model is using Adj Close as the label for training, what we need to record for evaluation\n",
    "# is the Open price for purchases and sales (assuming we are running the model overnight between two trade\n",
    "# days).The model updates and issues trades which will be filled at the next trading day open.\n",
    "\n",
    "# predictor can be \"pytorch_lstm\", \"random\", \"prescient\", \"drunk_genius\"\n",
    "\n",
    "def test_trading_system(model, test_seq, test_labels, D, Q, E, A, percentiles, predictor=\"pytorch_lstm\", drunk_genius_accuracy=0.5, basefn='', scale=1, offset=1, iter_per_seq=1, lr = 0.001, dr = 1, verbosity=0):\n",
    "    input_size = 6\n",
    "        \n",
    "#    print(basefn)\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = dr)\n",
    "    \n",
    "    batch_size = 1  # batch mode isn't used actually but including for future possible use\n",
    "            \n",
    "    holding = False;\n",
    "    journal = []   \n",
    "    total_profit_open = 0\n",
    "    total_profit_adj_close = 0\n",
    "    C = np.zeros(shape=(len(E)))\n",
    "    \n",
    "    # Keep list of predicted and actual returns and prices (for metrics reporting)\n",
    "    test_pred_rt = []\n",
    "    test_act_rt = []\n",
    "    test_pred_price = []\n",
    "    test_act_price = []    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trades_fn = basefn + '_trades.csv'\n",
    "    losses_fn = basefn + '_losses.csv'\n",
    "    \n",
    "    # Write headings for trades file\n",
    "    with open(trades_fn, 'w', newline='') as myfile:    \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Day','Adj Close','Pred Rt', 'Act Rt','Buy/Sell','Adj Cls Sell Price','Open Sell Price',\n",
    "                     'Bin','AdjCls Profit','Open Profit','Acc Profit Adj Cls','Acc Profit Open',\n",
    "                     'E0','E1','E2','E3','E4','E5','E6','E7',\n",
    "                     'A0','A1','A2','A3','A4','A5','A6','A7',                     \n",
    "                     'C0','C1','C2','C3','C4','C5','C6','C7',\n",
    "                     'Q0','Q1','Q2','Q3','Q4','Q5','Q6'])\n",
    "\n",
    "    # Write headings for losses file\n",
    "    with open(losses_fn, 'w', newline='') as myfile:    \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        heading_list=['Day','Trainloss','TestPred', 'TestAct', 'TestPredRt', 'TestActRt', 'TestLoss']\n",
    "        for i in range(len(test_seq[0])):\n",
    "            heading_list.append('TP'+str(i))\n",
    "        for i in range(len(test_seq[0])):\n",
    "            heading_list.append('TL'+str(i))            \n",
    "        for i in range(len(test_seq[0])):\n",
    "            heading_list.append('VP'+str(i))            \n",
    "        for i in range(len(test_seq[0])):\n",
    "            heading_list.append('VL'+str(i))                        \n",
    "        wr.writerow(heading_list)\n",
    "        \n",
    "    total_profit = 0\n",
    "    num_trades = 0\n",
    "    \n",
    "    for j in range(0, len(test_seq) - 2):      \n",
    "\n",
    "        # Send input / label data to Cuda if available\n",
    "        train_inputs = test_seq[j].to(device)\n",
    "        train_labels = test_labels[j].to(device)\n",
    "                \n",
    "#        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#        my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = dr)\n",
    "                \n",
    "\n",
    "        test_log = []\n",
    "        \n",
    "        # Reset the learning rate each time for the inner loop\n",
    "#        optimizer.param_groups[0]['lr'] = lr        \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # How many times we train on each sample as we go\n",
    "        for iter in range(iter_per_seq):\n",
    "\n",
    "            # Initialise hidden states before every epoch\n",
    "            h = model.init_hidden(batch_size)  \n",
    "                                    \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            train_y_pred, h, lstm_out = model(train_inputs, h, verbose=0)\n",
    "               \n",
    "            train_test_loss = loss_function(train_y_pred, train_labels)\n",
    "               \n",
    "            # Compute gradients and update the model\n",
    "            train_test_loss.backward()\n",
    "            optimizer.step()\n",
    "            my_lr_scheduler.step()\n",
    "                       \n",
    "        # Having updated the model with \"todays\" inputs, we now predict \"tomorrows\" return\n",
    "        model.eval()\n",
    "            \n",
    "        inputs = test_seq[j+1].to(device)\n",
    "        labels = test_labels[j+1].to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            h = model.init_hidden(batch_size)\n",
    "        \n",
    "            y_pred, h, lstm_out = model(inputs, h)\n",
    "            test_adj_close = inputs[-1][-1].item()                \n",
    "            test_pred = y_pred[-1].item()\n",
    "\n",
    "            test_truth = labels[-1].item()\n",
    "            test_actual_return = (denormalise_value(test_truth, scale, offset) / denormalise_value(test_adj_close, scale, offset)) - 1            \n",
    "            \n",
    "            # All trades are predicated on \"test_predicted_return\".  We mess with that here to implement the different kinds\n",
    "            # of predictor - \"pytorch_lstm\", \"random\" or \"prescient\". We save the actual predicted return though and update\n",
    "            # Q & Q with that, not the messed with value.\n",
    "            \n",
    "            pytorch_lstm_test_predicted_return =  (denormalise_value(test_pred, scale, offset) / denormalise_value(test_adj_close, scale, offset)) - 1\n",
    "            \n",
    "            if predictor == \"pytorch_lstm\":  # Usual behavior\n",
    "                test_predicted_return = pytorch_lstm_test_predicted_return\n",
    "            elif predictor == \"random\":\n",
    "                test_predicted_return = random.choice(D)   # pick a random return from D\n",
    "            elif predictor == \"prescient\":\n",
    "                test_predicted_return = test_actual_return    # Make the predicted return the actual return\n",
    "            elif predictor == \"drunk_genius\":   # Mix of prescient / random according to drunk_genius_accuracy\n",
    "                if random.random() <= drunk_genius_accuracy:\n",
    "                    test_predicted_return = test_actual_return\n",
    "                else:\n",
    "                    test_predicted_return = random.choice(D)   # pick a random return from D                \n",
    "\n",
    "            test_pred_price.append(test_pred)\n",
    "            test_act_price.append(test_truth)            \n",
    "            test_loss = (test_pred - test_truth)**2\n",
    "            \n",
    "            test_pred_rt.append(test_predicted_return)\n",
    "            test_act_rt.append(test_actual_return)\n",
    "            \n",
    "        if verbosity >= 4:\n",
    "            print(j, '{:1.2f}'.format(test_adj_close), '{:1.4f}'.format(test_predicted_return))\n",
    "            \n",
    "        test_log = [j, test_adj_close, test_predicted_return, test_actual_return]\n",
    "            \n",
    "        # Sell if return is negative or if it's the last day of trading and we're holding the asset\n",
    "        if ((test_predicted_return < 0 or j == len(test_seq) - 3)) and holding:\n",
    "            open_sell_price = denormalise_value(test_seq[j+2][-1][0].item(), scale, offset) # Next day open\n",
    "            sell_price = denormalise_value(test_adj_close, scale, offset)\n",
    "            profit = sell_price - purchase_price\n",
    "            total_profit += profit\n",
    "            num_trades += 1\n",
    "            E, A = update_EA(E, A, profit, purchase_bin)\n",
    "            if verbosity >= 4:\n",
    "                print(\"E\", E)\n",
    "                print(\"A\", A)\n",
    "            holding = False\n",
    "            \n",
    "            total_profit_open = total_profit_open + (open_sell_price - open_purchase_price)\n",
    "            total_profit_adj_close = total_profit_adj_close + profit\n",
    "            journal.append((-1, j+2+len(inputs), sell_price, total_profit_adj_close, open_sell_price, total_profit_open))\n",
    "            \n",
    "            if verbosity >= 3:\n",
    "                print(\"Executed sell at ${:1.2f}\".format(sell_price), \"on day\", j, \"Profit: ${:1.4f}\".format(profit),\n",
    "                     \"Total Profit: ${:1.4f}\".format(total_profit))\n",
    "                \n",
    "            test_log += (['Sell', sell_price, open_sell_price, purchase_bin, profit, open_sell_price - open_purchase_price, \n",
    "                          total_profit_adj_close, total_profit_open] + E.tolist() + A.tolist() + C.tolist() + Q.tolist())\n",
    "\n",
    "        # Buy\n",
    "        if test_predicted_return >= 0 and not holding:\n",
    "            return_bin = np.digitize(test_predicted_return, Q)\n",
    "            if verbosity >= 4:\n",
    "                print(\"Q\",Q,\"return_bin\", return_bin)\n",
    "            if A[return_bin] > 0:\n",
    "                purchase_bin = return_bin\n",
    "                C[purchase_bin] = C[purchase_bin] + 1\n",
    "                num_trades += 1\n",
    "                open_purchase_price = denormalise_value(test_seq[j+2][-1][0].item(), scale, offset) # Next day open\n",
    "                purchase_price = denormalise_value(test_adj_close, scale, offset)\n",
    "                holding = True\n",
    "                journal.append((1, j+2+len(inputs), purchase_price, total_profit_adj_close, open_purchase_price, total_profit_open))\n",
    "                \n",
    "                if verbosity >= 3:\n",
    "                    print(\"Executed buy at ${:1.2f}\".format(purchase_price), \"on day \", j, \"bin\", return_bin)\n",
    "                    \n",
    "                test_log += (['Buy', purchase_price, open_purchase_price, purchase_bin, 'N/A', 'N/A', \n",
    "                              total_profit_adj_close, total_profit_open] + E.tolist() + A.tolist() + C.tolist() + Q.tolist())\n",
    "\n",
    "        D, Q = update_DQ(D, Q, pytorch_lstm_test_predicted_return, percentiles)\n",
    "        \n",
    "        # Append the metrics to the training log file\n",
    "        with open(trades_fn, 'a', newline='') as myfile:    \n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(test_log)\n",
    "            \n",
    "        # Make a record of the training / test losses\n",
    "        with open(losses_fn, 'a', newline='') as losses_file:\n",
    "            wr = csv.writer(losses_file, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow([j,train_test_loss.item(),test_pred, test_truth, \n",
    "                         test_predicted_return, test_actual_return, test_loss] \n",
    "                        + train_y_pred.tolist() + train_labels.tolist() + y_pred.tolist() + labels.tolist())\n",
    "        \n",
    "\n",
    "    # The following is using Close - Open prices as start / end prices which is wrong and in all versions prior to 17\n",
    "    # Item 0 is the Open price\n",
    "#    start_price = denormalise_value(test_seq[0][-1][0].item(), scale, offset)\n",
    "\n",
    "    # Item 3 is the Close price\n",
    "#    end_price = denormalise_value(test_seq[-1][-1][3].item(), scale, offset)\n",
    "\n",
    "    # First row, Last item is the Adj Close \n",
    "    start_price = denormalise_value(test_seq[0][-1][-1].item(), scale, offset)\n",
    "\n",
    "    # Last row, Last item is the Adj Close\n",
    "    end_price = denormalise_value(test_seq[-1][-1][-1].item(), scale, offset)\n",
    "    \n",
    "    if len(journal):\n",
    "        test_profit = journal[-1][-3]\n",
    "        test_return_pct = 100 * journal[-1][-3]/start_price\n",
    "    else:\n",
    "        test_profit = 0\n",
    "        test_return_pct = 0\n",
    "\n",
    "    # Print a single line summary to screen\n",
    "    if verbosity == 1:\n",
    "        print(basefn, '{:1.2f}%'.format(test_return_pct), '{:3}'.format(num_trades) )\n",
    "        \n",
    "    # Print a detailed multi-line summary to screen\n",
    "    if verbosity >= 2:\n",
    "\n",
    "        print(\"Buy and Hold Baseline:\")\n",
    "        print(\"Price at start: ${:1.2f}\".format(start_price))\n",
    "        print(\"Price at end: ${:1.2f}\".format(end_price))\n",
    "        print(\"Difference: ${:1.2f}\".format(end_price - start_price))\n",
    "        print(\"Return:{:1.2f}%\".format(100*(end_price - start_price)/start_price))\n",
    "\n",
    "        print(\"\\nProfit from trading:{:1.2f}\".format(test_profit))\n",
    "        print(\"Return from trading:{:1.2f}%\".format(test_return_pct))\n",
    "        print(\"Num trades:{:3}\".format(num_trades))\n",
    "    \n",
    "        print(\"Length Returns:\", len(D))\n",
    "        print(\"Q:\\n\", Q)\n",
    "        print(\"E:\\n\", E)\n",
    "        print(\"A:\\n\", A)\n",
    "        print(\"C:\\n\", C)\n",
    "        \n",
    "        print(\"Test Time:\", (time.time()-start_time)/60)\n",
    "        \n",
    "    metrics_dict = compute_metrics(np.array(test_pred_rt),\n",
    "                                   np.array(test_act_rt),\n",
    "                                   np.array(test_pred_price),\n",
    "                                   np.array(test_act_price))\n",
    "\n",
    "    if verbosity >= 2:\n",
    "        print(\"N:\", metrics_dict['N'], \n",
    "              \"\\tMDA:{:1.2f}\".format(metrics_dict['MDA']), \n",
    "              \"\\tMAPE:{:1.2f}\".format(metrics_dict['MAPE']), \n",
    "              \"\\tMAE:{:1.5e}\".format(metrics_dict['MAE']), \n",
    "              \"\\tMSE:{:1.5e}\".format(metrics_dict['MSE']),\n",
    "              \"\\tPrice Correlation:{:1.2f}\".format(metrics_dict['PXCORR']),          \n",
    "              \"\\tReturns Correlation:{:1.2f}\".format(metrics_dict['RXCORR'])\n",
    "             )\n",
    "        \n",
    "    result_dict = {'bnh_profit': end_price - start_price, 'bnh_Return':100*(end_price - start_price)/start_price,\n",
    "              'num_trades':num_trades, \n",
    "              'trade_profit':test_profit, 'trade_return': test_return_pct, 'pred_rt':test_pred_rt, \n",
    "                  'act_rt':test_act_rt, 'pred_price':test_pred_price, 'act_price':test_act_price}\n",
    "\n",
    "    # Append the metrics to the training log file\n",
    "    with open(trades_fn, 'a', newline='') as myfile:    \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Num Samples For Metrics:', metrics_dict['N']])\n",
    "        wr.writerow(['MDA:', metrics_dict['MDA']])\n",
    "        wr.writerow(['MAPE:', metrics_dict['MAPE']])\n",
    "        wr.writerow(['MAE:', metrics_dict['MAE']])\n",
    "        wr.writerow(['MSE:', metrics_dict['MSE']])\n",
    "        wr.writerow(['Price Correlation:', metrics_dict['PXCORR']])\n",
    "        wr.writerow(['Returns Correlation:', metrics_dict['RXCORR']])\n",
    "        \n",
    "        \n",
    "        wr.writerow(['Start Price:', start_price])        \n",
    "        wr.writerow(['End Price:', end_price])\n",
    "        wr.writerow(['Difference $:', end_price-start_price])\n",
    "        wr.writerow(['Buy and Hold Return %:', 100*(end_price - start_price)/start_price])        \n",
    "        \n",
    "        wr.writerow(['Total profits from trading $:', test_profit])        \n",
    "        wr.writerow(['Return from trading %:', test_return_pct])\n",
    "        wr.writerow(['Total trades:', num_trades])\n",
    "        \n",
    "        wr.writerow(['Total Test Time:', (time.time()-start_time)/60])\n",
    "        \n",
    "    # Add the meytrics dict to the result dict\n",
    "    result_dict.update(metrics_dict)\n",
    "\n",
    "            \n",
    "    # return updated model, updated D, Q E and A, journal and results from this test period\n",
    "    return model, D, Q, E, A, journal, result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_allocation_policy(train_result_dict, percentiles, scale=1, offset=0, basefn='', verbosity=1):\n",
    "    \n",
    "    # Varios ways to avoid the initial training period where the model is way off\n",
    "    stop_idx = len(train_result_dict['val_pred_rt'])\n",
    "    # Just look at the last 120 time steps - very few data points though\n",
    "#    start_idx = stop_idx - 120\n",
    "\n",
    "    # Look at the last 80% of the time steps\n",
    "    start_idx = stop_idx - int(0.8 * stop_idx)\n",
    "    \n",
    "    # Point D at the training predicted returns - for the period we are interested in\n",
    "    D = np.array(train_result_dict['val_pred_rt'])[start_idx:stop_idx]\n",
    "\n",
    "    # Compute the percentiles of only the POSITIVE training returns\n",
    "    # This is what I was doing up to v12, changed in v13\n",
    "#    Q = np.percentile(D[D >= 0], percentiles)\n",
    "    \n",
    "    # Compute the percentiles of the absolute training returns, for the last N time steps\n",
    "    Q = np.percentile(abs(D), percentiles)\n",
    "       \n",
    "    if verbosity >= 1:\n",
    "        print(\"\\nComputing initial allocation policy from the training set...\")\n",
    "        print(\"Number of returns from training used for policy:\", len(D))\n",
    "        print(\"Initial Q:\\n\",Q)\n",
    "\n",
    "    # There will be 8 bins from 7 percentiles - anything below the lowest percentile is placed in bin 0\n",
    "    # Anything equal to or greater than the lowest percentile but less than the 2nd wil be in bin 1 etc\n",
    "    # Anything equal to or greater than the last percentile will be in the last bin.\n",
    "    \n",
    "    if verbosity >= 2:\n",
    "        binned_returns = np.digitize(D, Q)\n",
    "        unique, counts = np.unique(binned_returns, return_counts=True)\n",
    "        print(\"Unique bins:\", unique)\n",
    "        print(\"Counts / bin:\", counts)\n",
    "\n",
    "    # Generate the Expected Returns for each bin in Q from the training data\n",
    "    \n",
    "    holding = False\n",
    "    E = np.zeros(shape=(len(percentiles)+1))\n",
    "    C = np.zeros(shape=(len(percentiles)+1))    \n",
    "   \n",
    "    pred_rt = np.array(train_result_dict['val_pred_rt'])\n",
    "    act_rt = np.array(train_result_dict['val_act_rt'])\n",
    "    adj_close = np.array(train_result_dict['val_adj_close_price'])\n",
    "    \n",
    "    with open(basefn + '_init_alloc.csv', 'w', newline='') as myfile:    \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Day','Adj Close','Pred Rt', 'Act Rt','Buy/Sell','Sell Price','Bin','Acc Profit','Total Profit',\n",
    "                     'E0','E1','E2','E3','E4','E5','E6','E7',\n",
    "                     'C0','C1','C2','C3','C4','C5','C6','C7',\n",
    "                     'D0','D1','D2','D3','D4','D5','D6'])\n",
    "        \n",
    "    total_profit = 0\n",
    "\n",
    "    # Walk through the returns in the known valid region of the end of the training data\n",
    "    for i in range(start_idx, stop_idx):\n",
    "    \n",
    "        # This accumulates a line of values for the allocation log file\n",
    "        init_alloc_results = []\n",
    "        \n",
    "        if verbosity >= 2:\n",
    "            print(i, '{:1.2f}'.format(adj_close[i]), '{:1.4f}'.format(pred_rt[i]))\n",
    "        \n",
    "        init_alloc_results=[i,adj_close[i],pred_rt[i], act_rt[i]]\n",
    "        \n",
    "        # Sell\n",
    "        if pred_rt[i] < 0 and holding:\n",
    "            sell_price = denormalise_value(adj_close[i], scale, offset)\n",
    "            profit = sell_price - purchase_price\n",
    "            total_profit += profit\n",
    "            E[purchase_bin] = E[purchase_bin] + profit\n",
    "            holding = False\n",
    "            if verbosity >= 2:\n",
    "                print(\"Executed sell at ${:1.2f}\".format(sell_price), \"on day\", i, \"Profit: ${:1.4f}\".format(profit))\n",
    "            init_alloc_results += (['Sell', sell_price, purchase_bin, profit, total_profit] + E.tolist() + C.tolist() + Q.tolist())\n",
    "\n",
    "        # Buy\n",
    "        if pred_rt[i] >= 0 and not holding:\n",
    "            return_bin = np.digitize(pred_rt[i], Q)\n",
    "            purchase_bin = return_bin\n",
    "            C[purchase_bin] = C[purchase_bin] + 1\n",
    "            purchase_price = denormalise_value(adj_close[i], scale, offset)\n",
    "            holding = True\n",
    "            if verbosity >= 2:\n",
    "                print(\"Executed buy at ${:1.2f}\".format(purchase_price), \"on day \", i, \"bin\", return_bin)\n",
    "            init_alloc_results += (['Buy', purchase_price, purchase_bin, 'N/A', total_profit] + E.tolist() + C.tolist() + Q.tolist())\n",
    "            \n",
    "        # Append the metrics to the training log file\n",
    "        with open(basefn + '_init_alloc.csv', 'a', newline='') as myfile:    \n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(init_alloc_results)\n",
    "\n",
    "    \n",
    "    # It's important to understand that bin 0 won't have any trades at this point because we gave it the training\n",
    "    # data with a certain lower positive return. This value defines the lowest cut off of the training data. However,\n",
    "    # once we start getting new data we will see values probaby lower than this lowest bin cut off.\n",
    "    \n",
    "    A = (E > 0).astype(int)\n",
    "    \n",
    "    with open(basefn + '_init_alloc.csv', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([' ','A0','A1','A2','A3','A4','A5','A6','A7'])\n",
    "        wr.writerow(['Initial Allocation Policy:'] + A.tolist())\n",
    "                \n",
    "    if verbosity >= 1:\n",
    "        print(\"Expected Gains per Bin:\", E)\n",
    "        print(\"Trades per bin:\", C)\n",
    "        print(\"\\nInitial Allocation Policy:\",A)        \n",
    "        \n",
    "    return D,Q,E,A,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_dir - the root location of an existing run (including a saved_models directory) if we are picking up from a prevous run\n",
    "# resume_epoch - the epoch to resume from. Expect to find DQEAC_dictXXX.pkl and epochXXX.mdl files in a saved_models dir where\n",
    "#                XXX is the epoch number we wish to resume from.  -1 means start from the last.\n",
    "# train_epochs - how many epochs to train for (if starting from a pre-saved point, how many additonal epochs to train for)\n",
    "\n",
    "def run_hypers(train_start = '01-01-2005', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009',\n",
    "               test_start = '01-01-2010', test_end = '12-20-2019',  \n",
    "               resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=1, arch='LH', \n",
    "               num_layers=3, hidden_size=128, dropout=0.5, lr=0.01, dr=1.0, wd=0.0, trading_validation_period=100, validation_trials=10,\n",
    "               run_out_of_sample_test=0, train_only=0, verbosity=0, plot=0):\n",
    "\n",
    "    # If attempting to resume, check resume_dir/saved_models actually exists\n",
    "    if resume_dir != '':\n",
    "        if os.path.isdir(resume_dir + '/saved_models') == False:\n",
    "            print(\"Error: root directory for resume does not exist!\")\n",
    "            return\n",
    "    \n",
    "    hyper_results = []\n",
    "    \n",
    "    # Even if we're resuming, we will make a new directory for the new results\n",
    "    basefn = datetime.today().strftime('%Y-%m-%d-%H-%M-%S' + '/')\n",
    "    \n",
    "    # Make a results directory. If the 'directory' doesn't exist, create it\n",
    "    if os.path.isdir(basefn) == False:\n",
    "        os.mkdir(basefn)        \n",
    "                \n",
    "    hyper_results.append(basefn)\n",
    "    \n",
    "    # New additions to log file in CH15 - the root of a resumation and start epoch / how many epochs \n",
    "    # If resuming, put the location of the root directory. If starting from scratch put \"BASE\" in log field\n",
    "    if resume_dir=='':\n",
    "        hyper_results.append('BASE')\n",
    "    else:\n",
    "        hyper_results.append(resume_dir)\n",
    "        \n",
    "    hyper_results.append(resume_epoch)    \n",
    "    hyper_results.append(train_epochs)\n",
    "    \n",
    "    # Go get the data for the asset\n",
    "    abs_df = get_data(asset_name=asset, directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019')    \n",
    "    hyper_results.append(asset)\n",
    "\n",
    "    hyper_results.append(arch)        \n",
    "    hyper_results.append(iter_per_seq)\n",
    "    hyper_results.append(num_layers)        \n",
    "    hyper_results.append(window_size)        \n",
    "    hyper_results.append(hidden_size)        \n",
    "    hyper_results.append(lr)        \n",
    "    hyper_results.append(dr)\n",
    "    hyper_results.append(wd)        \n",
    "    hyper_results.append(dropout)      \n",
    "        \n",
    "    plotly_candlestick(abs_df, asset, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "    \n",
    "    # Add the Prev Adj Close, drop volume etc\n",
    "    abs_df = add_features_to_df(abs_df)\n",
    "    \n",
    "    # Normalise the entire dataset\n",
    "    df, scale, offset = normalise_df(abs_df)\n",
    "    \n",
    "    plotly_candlestick(df, asset, basefn=basefn, display_plot=plot, save_plot=1)    \n",
    "                \n",
    "        \n",
    "    # Mask off the training data and assign result to a train_asset dataframe\n",
    "    mask = (df['Date'] >= train_start) & (df['Date'] <= train_end)\n",
    "    train_asset_df = df.loc[mask]\n",
    "    \n",
    "    last_train_date = train_asset_df['Date'].iloc[-1]\n",
    "    print(\"last_train_date:\", last_train_date)\n",
    "    \n",
    "    loc_last_train_date = df.index[df['Date'] == last_train_date].tolist()[0]\n",
    "    print(\"loc_last_train_date:\", loc_last_train_date)\n",
    "    \n",
    "    backup_index = loc_last_train_date - window_size\n",
    "    print(\"backup_index:\", backup_index)\n",
    "    \n",
    "    backup_date = df['Date'].iloc[backup_index]\n",
    "    print(\"Backup date:\", backup_date)\n",
    "    \n",
    "    # Mask off the validation data and assign result to a valid_asset dataframe\n",
    "#    mask = (df['Date'] >= valid_start) & (df['Date'] <= valid_end)\n",
    "    mask = (df['Date'] >= backup_date) & (df['Date'] <= valid_end)\n",
    "    valid_asset_df = df.loc[mask]\n",
    "    \n",
    "    # Mask off the test data and assign result to a test_asset dataframe\n",
    "    mask = (df['Date'] >= test_start) & (df['Date'] <= test_end)\n",
    "    test_asset_df = df.loc[mask]\n",
    "        \n",
    "    # Reset the indices to start from 0 again for each of the dataframes\n",
    "    train_asset_df.index = np.arange(train_asset_df.shape[0])   \n",
    "    valid_asset_df.index = np.arange(valid_asset_df.shape[0])    \n",
    "    test_asset_df.index = np.arange(test_asset_df.shape[0]) \n",
    "    \n",
    "    # Generate validation attribute set\n",
    "    valid_seq, valid_labels = create_input_sequences_from_df(valid_asset_df, window_size)\n",
    "    \n",
    "    print(\"Train Start Date:\", train_asset_df['Date'].iloc[0])\n",
    "    print(\"Train End Date:\", train_asset_df['Date'].iloc[-1])\n",
    "    print(\"Train Length:\", len(train_asset_df))\n",
    "    print(\"Valid Start Date:\", valid_asset_df['Date'].iloc[0])\n",
    "    print(\"Valid End Date:\", valid_asset_df['Date'].iloc[-1])\n",
    "    print(\"Valid Length:\", len(valid_asset_df))\n",
    "\n",
    "    # Convert our list of lists to Tensors\n",
    "    valid_seq = torch.FloatTensor(valid_seq)\n",
    "    valid_labels = torch.FloatTensor(valid_labels)\n",
    "    \n",
    "        \n",
    "    percentiles = [0,10,20,30,40,50,60]\n",
    "#    percentiles = [0,10,20,30,40,50,60,70,80,90]    \n",
    "       \n",
    "    model, train_result_dict = create_train_model(train_asset_df, resume_dir=resume_dir, resume_epoch=resume_epoch, \n",
    "                                            basefn=basefn, scale=scale, offset=offset, arch=arch, \n",
    "                                            window_size = window_size, epochs=train_epochs,\n",
    "                                            iter_per_seq=iter_per_seq, max_iter=1E32, num_layers=num_layers,\n",
    "                                            hidden_size=hidden_size, dropout=dropout, lr=lr, dr=dr, wd=wd,\n",
    "                                            trading_validation_period=trading_validation_period,\n",
    "                                            validation_trials=validation_trials,\n",
    "                                            percentiles=percentiles,\n",
    "                                            valid_seq=valid_seq, valid_labels=valid_labels,\n",
    "                                            verbosity=verbosity, plot=plot)        \n",
    "\n",
    "    # Append the metrics to the training log file\n",
    "    with open(basefn + 'train.csv', 'a', newline='') as myfile:    \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Num Samples For Metrics:', train_result_dict['trStats_N']])\n",
    "        wr.writerow(['MDA:', train_result_dict['trMDA']])\n",
    "        wr.writerow(['MAPE:', train_result_dict['trMAPE']])\n",
    "        wr.writerow(['MAE:', train_result_dict['trMAE']])\n",
    "        wr.writerow(['AveTrMSE:', train_result_dict['trAvTrMSE']])        \n",
    "        wr.writerow(['AveValMSE:', train_result_dict['trAvValMSE']])\n",
    "        wr.writerow(['Price X-Correl:', train_result_dict['trPXCORR']])        \n",
    "        wr.writerow(['Returns X-Correl:', train_result_dict['trRXCORR']])    \n",
    "        \n",
    "    hyper_results.append(train_result_dict['trAvTrMSE'])\n",
    "    hyper_results.append(train_result_dict['trAvValMSE'])    \n",
    "    hyper_results.append(train_result_dict['trMDA'])\n",
    "    \n",
    "    if train_only:\n",
    "        return model, [], train_result_dict\n",
    "    \n",
    "    # MODEL TRAINING COMPLETE - NOW WE BUILD THE INITIAL ALLOCATION POLICY\n",
    "        \n",
    "    D,Q,E,A,C = generate_initial_allocation_policy(train_result_dict, percentiles, scale=scale, offset=offset, basefn=basefn, verbosity=verbosity)\n",
    "\n",
    "    policy_dict = {'initial_D':D,'initial_Q':Q,'initial_E':E,'initial_A':A,'initial_C':C}\n",
    "                   \n",
    "        \n",
    "    print(\"\\nPERFORMING VALIDATION...\")\n",
    "        \n",
    "    model, D, Q, E, A, journal, test_result_dict = test_trading_system(model, valid_seq, valid_labels, D, Q, E, A, percentiles, basefn=basefn+'_val', scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = 0.001, dr = 1, verbosity=verbosity)\n",
    "\n",
    "    result_dict = {'val_profit':test_result_dict['trade_profit'], 'val_return_pct':test_result_dict['trade_return']}\n",
    "        \n",
    "    hyper_results.append(test_result_dict['MDA'])    \n",
    "    hyper_results.append(test_result_dict['MSE'])\n",
    "    hyper_results.append(test_result_dict['num_trades'])    \n",
    "    hyper_results.append(test_result_dict['PXCORR'])\n",
    "    hyper_results.append(test_result_dict['RXCORR'])              \n",
    "    hyper_results.append(test_result_dict['trade_profit'])\n",
    "    hyper_results.append(test_result_dict['trade_return'])         \n",
    "    \n",
    "    policy_dict['post_val_D'] = D\n",
    "    policy_dict['post_val_Q'] = Q\n",
    "    policy_dict['post_val_E'] = E\n",
    "    policy_dict['post_val_A'] = A\n",
    "                \n",
    "    if run_out_of_sample_test:\n",
    "        \n",
    "        if verbosity >= 1:\n",
    "            print(\"\\nEVALUATING TEST SET...\")    \n",
    "    \n",
    "        # Generate test attribute set\n",
    "        test_seq, test_labels = create_input_sequences_from_df(test_asset_df, window_size)\n",
    "\n",
    "        # Convert our list of lists to Tensors\n",
    "        test_seq = torch.FloatTensor(test_seq)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "\n",
    "        model, D, Q, E, A, journal, test_result_dict = test_trading_system(model, test_seq, test_labels, D, Q, E, A, percentiles, basefn=basefn+'test', scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = 0.001, dr = 1, verbosity=verbosity)\n",
    "    \n",
    "        result_dict['test_profit'] = test_result_dict['trade_profit']\n",
    "        result_dict['test_return_pct'] = test_result_dict['trade_return']\n",
    "        \n",
    "        policy_dict['post_test_D'] = D\n",
    "        policy_dict['post_test_Q'] = Q\n",
    "        policy_dict['post_test_E'] = E\n",
    "        policy_dict['post_test_A'] = A\n",
    "        \n",
    "    with open('hyper_log.csv', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(hyper_results + policy_dict['initial_E'].tolist() + policy_dict['initial_C'].tolist() + policy_dict['initial_A'].tolist())\n",
    "        \n",
    "    with open(basefn + 'policy_dict.pkl','wb') as fp:\n",
    "        pickle.dump(policy_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "    return model, journal, result_dict, policy_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 22/3/21 CH17 laptop - See if we can take a saved model, run 1 epoch to warm it up and do a validation without \n",
    "# the discontinuity at the beginning\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='01-01-2008', valid_start='01-02-2008', valid_end='12-31-2009', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see if the same model works in the first test period - 2010-2011\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='12-31-2009', valid_start='01-01-2010', valid_end='12-31-2011', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's see if the same model works in the next test period - 2012-2013\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='12-31-2011', valid_start='01-01-2012', valid_end='12-31-2013', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see if the same model works in the period 2014-2015\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='12-31-2013', valid_start='01-01-2014', valid_end='12-31-2015', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "\n",
    "# Now let's see if the same model works in the period 2016-2017\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='12-31-2015', valid_start='01-01-2016', valid_end='12-31-2017', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "\n",
    "# Now let's see if the same model works in the period 2016-2017\n",
    "resume_dir = 'CH17 - Exp5 - Hyper-tuning window size/w=33 - 2021-03-21-00-27-24'\n",
    "model, journal, result, policy_dict = run_hypers(train_start='01-01-2005', train_end='12-31-2017', valid_start='01-01-2018', valid_end='12-20-2019', resume_dir=resume_dir, resume_epoch=400, window_size=33, asset='SPY', train_epochs=1, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=1, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '06-30-2009', valid_start = '07-01-2009', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2008', train_end = '06-30-2011', valid_start = '07-01-2011', valid_end = '12-31-2011', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2010', train_end = '06-30-2013', valid_start = '07-01-2013', valid_end = '12-31-2013', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2012', train_end = '06-30-2015', valid_start = '07-01-2015', valid_end = '12-31-2015', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2014', train_end = '06-30-2017', valid_start = '07-01-2017', valid_end = '12-31-2018', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, validation_trials=25, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2015', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=22, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=44, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=32, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2006', train_end = '01-01-2009', valid_start = '01-02-2009', valid_end = '12-31-2010', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This rolls forward the training / validaton period 1 year at a time from 2005 through 2015\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2005', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# model, journal, result, policy_dict = run_hypers(train_start = '01-01-2006', train_end = '01-01-2009', valid_start = '01-02-2009', valid_end = '12-31-2010', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2007', train_end = '01-01-2010', valid_start = '01-02-2010', valid_end = '12-31-2011', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# model, journal, result, policy_dict = run_hypers(train_start = '01-01-2008', train_end = '01-01-2011', valid_start = '01-02-2011', valid_end = '12-31-2012', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2009', train_end = '01-01-2012', valid_start = '01-02-2012', valid_end = '12-31-2013', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# model, journal, result, policy_dict = run_hypers(train_start = '01-01-2010', train_end = '01-01-2013', valid_start = '01-02-2013', valid_end = '12-31-2014', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2011', train_end = '01-01-2014', valid_start = '01-02-2014', valid_end = '12-31-2015', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# model, journal, result, policy_dict = run_hypers(train_start = '01-01-2012', train_end = '01-01-2015', valid_start = '01-02-2015', valid_end = '12-31-2016', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2013', train_end = '01-01-2016', valid_start = '01-02-2016', valid_end = '12-31-2017', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# model, journal, result, policy_dict = run_hypers(train_start = '01-01-2014', train_end = '01-01-2017', valid_start = '01-02-2017', valid_end = '12-31-2018', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2015', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This expands the training period before 2008-2009 validation in 0.5 year increments from 0.5 years\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2007', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2007', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2005', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "# The last one is standard Chalvatzis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning - window size\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=22, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=44, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=55, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=55, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter Hidden Size - 21/3/21\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=32, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=96, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=160, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-tune DR to see if we can converge a model \n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=0.999, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=0.997, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=0.995, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=0.993, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '07-01-2006', train_end = '01-01-2008', valid_start = '01-02-2008', valid_end = '12-31-2009', resume_dir='', resume_epoch=-1, window_size=33, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=0.991, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2012', train_end = '01-01-2015', valid_start = '01-02-2015', valid_end = '12-31-2016', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2013', train_end = '01-01-2016', valid_start = '01-02-2016', valid_end = '12-31-2017', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2014', train_end = '01-01-2017', valid_start = '01-02-2017', valid_end = '12-31-2018', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2015', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extends the training period while keeping a constant length validation period (2018-2019)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2015', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2014', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2013', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2012', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2011', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=3500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result, policy_dict = run_hypers(train_start = '01-01-2010', train_end = '01-01-2018', valid_start = '01-02-2018', valid_end = '12-18-2019', resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=4000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=22, asset='SPY', train_epochs=3000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=32, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(resume_dir='', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-3, dr=0.995, wd=0, trading_validation_period=50, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of resuming training on a previous run\n",
    "model, journal, result, policy_dict = run_hypers(resume_dir='CH14 - Exp 2 - 2000 epochs/2021-03-01-20-54-12', resume_epoch=-1, window_size=11, asset='SPY', train_epochs=2000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=10, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_out_of_sample_test(model, D, Q, E, A, basefn='', asset='SPY', window_size=11, iter_per_seq=3, lr=1E-3, dr=0):\n",
    "    \n",
    "    # Go get the data for the asset\n",
    "    abs_df = get_data(asset_name=asset, directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019')\n",
    "\n",
    "    # Add the Prev Adj Close, drop volume etc\n",
    "    abs_df = add_features_to_df(abs_df)\n",
    "    \n",
    "    # Normalise the entire dataset\n",
    "    df, scale, offset = normalise_df(abs_df)\n",
    "    \n",
    "    # Out of sample test set\n",
    "    test_start = '01-01-2010'\n",
    "    test_end = '12-20-2019'\n",
    "    \n",
    "#    test_asset_dict = {}\n",
    "       \n",
    "#    backup_index = len(test_asset_df) - window_size\n",
    "#    backup_date = df['Date'].iloc[backup_index]\n",
    "    \n",
    "    # Mask off the test data and assign result to a test_asset dataframe\n",
    "    mask = (df['Date'] >= test_start) & (df['Date'] <= test_end)\n",
    "    test_asset_df = df.loc[mask]\n",
    "    \n",
    "    # Reset the indices to start from 0 again for each of the dataframes\n",
    "    test_asset_df.index = np.arange(test_asset_df.shape[0]) \n",
    "    \n",
    "    # Generate test attribute set\n",
    "    test_seq, test_labels = create_input_sequences_from_df(test_asset_df, window_size)\n",
    "\n",
    "    # Convert our list of lists to Tensors\n",
    "    test_seq = torch.FloatTensor(test_seq)\n",
    "    test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "    percentiles = [0,10,20,30,40,50,60]\n",
    "\n",
    "    model, D, Q, E, A, journal, test_result_dict = test_trading_system(model, test_seq, test_labels, D, Q, E, A, percentiles, basefn=basefn, scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = lr, dr = dr, verbosity=0)\n",
    "    \n",
    "    return model, D, Q, E, A, test_result_dict\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trading_validation(model, D, Q, E, A, predictor=\"pytorch_lstm\", drunk_genius_accuracy=0.5, valid_start='01-02-2008', valid_end='12-31-2009', basefn='', asset='SPY', window_size=11, iter_per_seq=3, lr=1E-3, dr=0):\n",
    "    \n",
    "    \n",
    "    # Go get the data for the asset\n",
    "    abs_df = get_data(asset_name=asset, directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019')\n",
    "\n",
    "    # Add the Prev Adj Close, drop volume etc\n",
    "    abs_df = add_features_to_df(abs_df)\n",
    "    \n",
    "    # Normalise the entire dataset\n",
    "    df, scale, offset = normalise_df(abs_df)\n",
    "    \n",
    "    # Training set\n",
    "#    train_start = '01-01-2005'\n",
    "#    train_end = '01-01-2008'    \n",
    "            \n",
    "    # Mask off the training data and assign result to a train_asset dataframe\n",
    "#    mask = (df['Date'] >= train_start) & (df['Date'] <= train_end)\n",
    "#    train_asset_df = df.loc[mask]\n",
    "    \n",
    "#    backup_index = len(train_asset_df) - window_size\n",
    "#    backup_date = df['Date'].iloc[backup_index]\n",
    "    \n",
    "    # Mask off the validation data and assign result to a valid_asset dataframe\n",
    "\n",
    "    mask = (df['Date'] >= valid_start) & (df['Date'] <= valid_end)\n",
    "    valid_asset_df = df.loc[mask]\n",
    "           \n",
    "    # Reset the indices to start from 0 again for each of the dataframes\n",
    "#    train_asset_df.index = np.arange(train_asset_df.shape[0])   \n",
    "    valid_asset_df.index = np.arange(valid_asset_df.shape[0])    \n",
    "    \n",
    "    print(\"Valid Start Date:\", valid_asset_df['Date'].iloc[0])\n",
    "    print(\"Valid End Date:\", valid_asset_df['Date'].iloc[-1])\n",
    "    print(\"Valid Length:\", len(valid_asset_df))\n",
    "\n",
    "    # Generate validation attribute set\n",
    "    valid_seq, valid_labels = create_input_sequences_from_df(valid_asset_df, window_size)\n",
    "\n",
    "    # Convert our list of lists to Tensors\n",
    "    valid_seq = torch.FloatTensor(valid_seq)\n",
    "    valid_labels = torch.FloatTensor(valid_labels)\n",
    "                    \n",
    "    percentiles = [0,10,20,30,40,50,60]\n",
    "\n",
    "    model, D, Q, E, A, journal, test_result_dict = test_trading_system(model, valid_seq, valid_labels, D, Q, E, A, percentiles, \n",
    "                        predictor=predictor, drunk_genius_accuracy=drunk_genius_accuracy, basefn=basefn, scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = lr, \n",
    "                        dr = dr, verbosity=0)\n",
    "    \n",
    "#    print(test_result_dict)\n",
    "    return model, D, Q, E, A, test_result_dict\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function runs a validation / test repeatability evaluation on a saved model from training\n",
    "\n",
    "def run_trading_trials(predictor='pytorch_lstm', drunk_genius_accuracy=0.5, valid_start='01-02-2008', valid_end='12-31-2009',savedfn='', model_num=-1, num_trials=10, run_test=0, iter_per_seq=3, lr=1E-3, dr=0):\n",
    "\n",
    "    model_fn = savedfn + 'epoch' + str(model_num) + '.mdl'\n",
    "    print(model_fn)\n",
    "    \n",
    "    DQEA_fn = savedfn + 'DQEAC_dict' + str(model_num) + '.pkl'\n",
    "    print(DQEA_fn)\n",
    "    \n",
    "    resultsfn = datetime.today().strftime('%Y-%m-%d-%H-%M-%S' + '/')\n",
    "    \n",
    "    # Make a results directory. If the 'directory' doesn't exist, create it\n",
    "    if os.path.isdir(resultsfn) == False:\n",
    "        os.mkdir(resultsfn)       \n",
    "\n",
    "    with open(resultsfn + 'val_results.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['N', 'iter_per_seq', 'lr', 'dr', 'num_trades', 'trade_return'])     \n",
    "\n",
    "    with open(resultsfn + 'test_results.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['N', 'num_trades', 'trade_return'])\n",
    "            \n",
    "    with open(resultsfn + 'val_test_summary.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['N','iter_per_seq', 'lr', 'dr', 'mean_val_trades','mean_val_returns','std_val_returns','min_val_returns',\n",
    "                     'max_val_returns', 'mean_MDA', 'mean_RtCorr', 'mean_MSE', 'mean_MAE', 'mean_MAPE',\n",
    "                     'mean_test_trades', 'mean_test_returns', 'std_test_returns', 'min_test_returns',\n",
    "                     'max_test_returns'])\n",
    "    \n",
    "    val_trades = []\n",
    "    test_trades = []\n",
    "    val_mda = []\n",
    "    val_mae = []\n",
    "    val_mape = []\n",
    "    val_rxcorr = []\n",
    "    val_mse = []\n",
    "    val_returns = []\n",
    "    test_returns = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        saved_model = torch.load(model_fn)\n",
    "    \n",
    "        with open(DQEA_fn, 'rb') as fp:\n",
    "            saved_DQEAC = pickle.load(fp)\n",
    "        \n",
    "        D, Q, E, A, C = saved_DQEAC['D'], saved_DQEAC['Q'], saved_DQEAC['E'], saved_DQEAC['A'], saved_DQEAC['C']    \n",
    "    \n",
    "        model, D, Q, E, A, result_dict = run_trading_validation(saved_model, D, Q, E, A, predictor=predictor, \n",
    "                                                                drunk_genius_accuracy=drunk_genius_accuracy,\n",
    "                                                                valid_start=valid_start, valid_end=valid_end, \n",
    "                                                                basefn=resultsfn+str(i)+'_', iter_per_seq=iter_per_seq, \n",
    "                                                                lr=lr, dr=dr)\n",
    "        \n",
    "        print(\"Val:\", i, iter_per_seq, lr,dr, result_dict['num_trades'], result_dict['trade_return'], \n",
    "              result_dict['MDA'], result_dict['RXCORR'], result_dict['MSE'])\n",
    "        \n",
    "        val_returns.append(result_dict['trade_return'])\n",
    "        val_trades.append(result_dict['num_trades'])\n",
    "        val_mda.append(result_dict['MDA'])\n",
    "        val_mae.append(result_dict['MAE'])        \n",
    "        val_mape.append(result_dict['MAPE'])                \n",
    "        val_rxcorr.append(result_dict['RXCORR'])\n",
    "        val_mse.append(result_dict['MSE'])\n",
    "        \n",
    "        with open(resultsfn + 'val_results.csv', 'a', newline='') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow([i, iter_per_seq, lr, dr, result_dict['num_trades'],result_dict['trade_return'], result_dict['MDA'],\n",
    "                        result_dict['RXCORR'], result_dict['MSE'], result_dict['MAE'], result_dict['MAPE']])\n",
    "    \n",
    "        if run_test:\n",
    "            model, D, Q, E, A, test_result_dict = run_out_of_sample_test(model, D, Q, E, A, basefn=resultsfn+str(i)+'_', iter_per_seq=iter_per_seq, lr=lr, dr=dr)\n",
    "            print(\"Test:\", i, test_result_dict['num_trades'], test_result_dict['trade_return'])\n",
    "            test_returns.append(test_result_dict['trade_return'])\n",
    "            test_trades.append(test_result_dict['num_trades'])    \n",
    "        \n",
    "            with open(resultsfn + 'test_results.csv', 'a', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow([i,test_result_dict['num_trades'],test_result_dict['trade_return']])\n",
    "\n",
    "    if run_test:\n",
    "        print(model_num, np.mean(val_trades),np.mean(val_returns), np.mean(test_trades),np.mean(test_returns))\n",
    "    else:\n",
    "        print(model_num, np.mean(val_trades),np.mean(val_returns))\n",
    "    \n",
    "    with open(resultsfn + 'val_test_summary.csv', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        if run_test:\n",
    "            wr.writerow([num_trials,iter_per_seq, lr, dr,np.mean(val_trades),np.mean(val_returns),np.std(val_returns),\n",
    "                         np.min(val_returns),np.max(val_returns), np.mean(val_mda), np.mean(val_rxcorr), np.mean(val_mse),\n",
    "                         np.mean(val_mae), np.mean(val_mape), \n",
    "                         np.mean(test_trades),np.mean(test_returns),np.std(test_returns),np.min(test_returns),np.max(test_returns)])\n",
    "        else:\n",
    "            wr.writerow([i,iter_per_seq, lr, dr,np.mean(val_trades),np.mean(val_returns),np.std(val_returns),np.min(val_returns),\n",
    "                         np.max(val_returns), np.mean(val_mda), np.mean(val_rxcorr), np.mean(val_mse), np.mean(val_mae), \n",
    "                        np.mean(val_mape)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the last 2 years of the rolling period\n",
    "# savedfn = 'CH15 - Exp7 - Roll forward train validation period/2018 - 2021-03-16-00-08-34/saved_models/'\n",
    "# run_trading_trials(valid_start='01-01-2018', valid_end='12-19-2019',savedfn=savedfn, model_num=900, num_trials=10, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# This is the best performing period and model I saw in the rolling experiment\n",
    "# saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2009 - 2021-03-13-14-20-53/saved_models'\n",
    "#run_trading_trials(valid_start='12-15-2006', valid_end='12-31-2008',savedfn=savedfn, model_num=850, num_trials=10, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# \n",
    "saved_fn = 'Ch15 - CH16 - Exp8 - Expanding training period/3 year partial - 2021-03-16-13-05-13/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2017', valid_end='12-18-2019',savedfn=savedfn, model_num=750, num_trials=25, run_test=0, iter_per_seq=8, lr=1E-4, dr=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_fn = 'Ch15 - CH16 - Exp8 - Expanding training period/3 year partial - 2021-03-16-13-05-13/saved_models'\n",
    "for drunk_genius_accuracy in np.arange(0,1.1,0.1):\n",
    "    run_trading_trials(predictor=\"drunk_genius\", drunk_genius_accuracy=drunk_genius_accuracy, valid_start='12-13-2017', valid_end='12-18-2019',savedfn=savedfn, model_num=750, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2008\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2008 - 2021-03-03-17-09-15 LR1E-4 LRD0 - baseline from CH15 Exp2/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-14-2007', valid_end='12-31-2009',savedfn=savedfn, model_num=1050, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2009\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2009 - 2021-03-13-14-20-53/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2008', valid_end='12-31-2010',savedfn=savedfn, model_num=850, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2010\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2010 - 2021-03-13-21-37-25/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2009', valid_end='12-31-2011',savedfn=savedfn, model_num=650, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2011\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2011 - 2021-03-14-04-34-53/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2010', valid_end='12-31-2012',savedfn=savedfn, model_num=700, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2012\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2012 - 2021-03-14-11-33-41/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2011', valid_end='12-31-2013',savedfn=savedfn, model_num=450, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2013\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2013 - 2021-03-14-18-33-45/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2012', valid_end='12-31-2014',savedfn=savedfn, model_num=650, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2014\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2014 - 2021-03-15-01-30-56/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2013', valid_end='12-31-2015',savedfn=savedfn, model_num=1050, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2015\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2015 - 2021-03-15-08-29-15/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2014', valid_end='12-31-2016',savedfn=savedfn, model_num=650, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2016\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2016 - 2021-03-15-13-41-40/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2015', valid_end='12-31-2017',savedfn=savedfn, model_num=900, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2017\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2017 - 2021-03-15-18-54-04/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2016', valid_end='12-31-2018',savedfn=savedfn, model_num=300, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n",
    "# 2018\n",
    "saved_fn = 'CH15 - Exp7 - Roll forward train validation period/2018 - 2021-03-16-00-08-34/saved_models'\n",
    "run_trading_trials(predictor=\"pytorch_lstm\", valid_start='12-13-2017', valid_end='12-31-2019',savedfn=savedfn, model_num=900, num_trials=25, run_test=0, iter_per_seq=3, lr=1E-3, dr=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result, policy_dict = run_hypers(window_size=11, asset='SPY', train_epochs=4000, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=10, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('policy_dict.pkl','wb') as fp:\n",
    "    pickle.dump(policy_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=10, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=10, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, trading_validation_period=10, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=2, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=4, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=5, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=6, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.5, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.99, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trial in range(100):\n",
    "    iter_per_seq = random.choice([10, 20, 30, 40, 50, 60 ,70, 80, 90, 100])\n",
    "    window_size = random.choice([11, 22, 44])\n",
    "    hidden_size = random.choice([32, 64, 128])\n",
    "    num_layers = random.choice([2, 3])\n",
    "    arch = random.choice(['LH','AH'])\n",
    "    dropout = random.choice([0, 0.5, 0.7])\n",
    "    lr = random.choice([1E-3, 5E-4, 1E-4, 5E-5, 1E-5])\n",
    "    dr = random.choice([0.9, 0.93, 0.95, 0.97, 0.99, 1])\n",
    "    model, journal, result = run_hypers(window_size=window_size, asset='SPY', train_epochs=1, iter_per_seq=iter_per_seq, arch=arch, num_layers=num_layers, hidden_size=hidden_size, dropout=dropout, lr=lr, dr=dr, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in range(10):\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=40, arch='AH', num_layers=2, hidden_size=64, dropout=0.7, lr=1E-4, dr=0.95, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in range(10):\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.7, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)\n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.1, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.2, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.3, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.4, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.5, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.6, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.7, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.8, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=0.9, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    \n",
    "model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=2, hidden_size=32, dropout=1, lr=1E-3, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=1, plot=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=2, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=3, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=4, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=5, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1500, iter_per_seq=6, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=1E-4, dr=1, wd=0, run_out_of_sample_test=0, train_only=0, verbosity=2, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=1, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.9, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.9, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.9, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.9, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.9, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.90, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.91, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.92, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.93, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.94, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.96, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.97, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.98, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.90, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.91, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.92, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.93, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.94, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.96, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.97, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.98, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.99, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.00001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.000001, dr=0.95, wd=0, run_out_of_sample_test=0, verbosity=1, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=0, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-8, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-7, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-5, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-4, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=0.999, wd=1E-3, run_out_of_sample_test=0, verbosity=2, plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r1 - r8 run on 16/2/21-18/2/21\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.0001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.0001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=11, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.0001, dr=1, wd=5E-6, run_out_of_sample_test=0, verbosity=2, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_synthetic_stock(source_asset_name='SPY', new_name='SYN', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', trend_list=[(10,37),(7,17),(3,7)], adj_close_start=85, price_slope=0, close_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_synthetic_stock(source_asset_name='SPY', new_name='SYN2', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', trend_list=[(10,33),(5,17)], adj_close_start=85, price_slope=0, close_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_synthetic_stock(source_asset_name='SPY', new_name='SYN3', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', trend_list=[(10,33),(5,17)], adj_close_start=85, price_slope=0.01, close_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_synthetic_stock(source_asset_name='SPY', new_name='SYN4', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='12-20-2019', trend_list=[(10,33),(5,17),(3,11),(2,3)], adj_close_start=85, price_slope=0.01, close_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_synthetic_stock(source_asset_name='SPY', new_name='SYN4S', directory='../../../data/yahoo_data', start_date='01-01-2005', stop_date='03-31-2005', trend_list=[(10,33),(5,17),(3,11),(2,3)], adj_close_start=85, price_slope=0.01, close_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_win_leg()\n",
    "epochs_win_leg()\n",
    "iterations_lr_leg()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPY Num Iterations Leg - using optimum settings from SYN4 experiments\n",
    "def spy_iterations_leg():\n",
    "    %mkdir \"num_iter\"\n",
    "    %cd num_iter\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=50, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=200, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=300, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=400, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    %cd ..\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPY Num Iterations Leg - using optimum settings from SYN4 experiments\n",
    "def spy_iterations_leg2():\n",
    "    %mkdir \"num_iter2\"\n",
    "    %cd num_iter\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1100, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1200, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1300, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1400, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=1500, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "#    model, journal, result = run_hypers(window_size=44, asset='SPY', train_epochs=1, iter_per_seq=400, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spy_iterations_leg2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num Iterations Leg - using optimum settings from SYN4 experiments\n",
    "def iterations_leg():\n",
    "    %mkdir \"num_iter\"\n",
    "    %cd num_iter\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=5, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=20, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=40, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=80, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    %cd ..\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_leg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterations, Window size leg\n",
    "def iterations_win_leg():\n",
    "    %mkdir \"iter_win_len\"\n",
    "    %cd iter_win_len\n",
    "    model, journal, result = run_hypers(window_size=1, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=2, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=3, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=7, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=11, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=88, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    %cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs, Window size leg\n",
    "def epochs_win_leg():\n",
    "    %mkdir \"epoch_win_len\"\n",
    "    %cd epoch_win_len\n",
    "\n",
    "    model, journal, result = run_hypers(window_size=1, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=2, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=3, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=7, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=11, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=44, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=88, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterations, Learning rate leg\n",
    "def iterations_lr_leg():\n",
    "    %mkdir \"iter_lr\"\n",
    "    %cd iter_lr\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.000001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.00001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.01, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.1, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch, Learning rate leg\n",
    "def epoch_lr_leg():\n",
    "    %mkdir \"epoch_lr\"\n",
    "    %cd epoch_lr\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.000001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.00001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.0001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.01, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.1, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    \n",
    "    %cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterations, Hidden size leg\n",
    "def iterations_hidden_size_leg():\n",
    "    %mkdir \"iter_hidden_len\"\n",
    "    %cd iter_hidden_len\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=2, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=4, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=8, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=16, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=32, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=1, iter_per_seq=10, arch='LH', num_layers=3, hidden_size=256, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    \n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs, Hidden size leg\n",
    "def epochs_hidden_size_leg():\n",
    "    %mkdir \"epoch_hidden_len\"\n",
    "    %cd epoch_hidden_len\n",
    "    \n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=2, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=4, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=8, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=16, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=32, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=64, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=128, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    model, journal, result = run_hypers(window_size=22, asset='SYN4', train_epochs=10, iter_per_seq=1, arch='LH', num_layers=3, hidden_size=256, dropout=0.5, lr=0.001, dr=1, run_out_of_sample_test=0, verbosity=1, plot=1)\n",
    "    \n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything past here is graveyard / experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE OLD (SLOW) VERSION OF TRAIN_MODEL - IT WAS REPLACED BY TRAIN_MODELQ\n",
    "\n",
    "# Trains the given LSTM model. Also collects validation data (predicting the next day) and collects stats\n",
    "# for both.  These are used to observe training vs validation but also to construct the first allocation policy.\n",
    "\n",
    "def train_model(model, train_seq, train_labels, scale=1, offset=1, train_start=0, train_length=1, epochs = 20, iter_per_seq=1, max_iter=1600, lr = 0.001, dr=0.999, wd=0.0, verbosity=0, basefn='',percentiles=[0,10,20,30,40,50,60], trading_validation_period=100, valid_seq=[], valid_labels=[], plot=0):\n",
    "    input_size = 6\n",
    "    \n",
    "    \n",
    "    # if plot level is 4, plot graphs on screen for every inner loop training run\n",
    "    if plot == 4:\n",
    "        dump_log_interval = 1\n",
    "    else:\n",
    "        dump_log_interval = 10 # otherwise do it every 10\n",
    "        \n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = dr)\n",
    "    \n",
    "    batch_size = 1  # batch mode isn't used actually but including for future possible use\n",
    "    \n",
    "    with open(basefn + 'train.csv', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Epc','AvTrLoss','AvTrMSE','AvValMSE','MdnTrMSE','MdnValMSE','ValMDA','ValMAPE','ValMAE','PXcorr','RXcorr',\n",
    "                     'EpcSecs','TtlMins'])\n",
    "        \n",
    "    with open(basefn + 'trade_validation.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Epoch','MDA','MSE','MAE','MAPE','X-Correl','Num-trades','AveRt%','MdnRT%','MinRt%','MaxRt%',\n",
    "                    'RangeRt%','StdDevRt%'])\n",
    "            \n",
    "    # Outputs a summary line for each Epoch\n",
    "    if verbosity == 1:\n",
    "        print('Epc  AvTrLoss  AvTrMSE   AvValMSE  MdnTrMSE  MdnValMSE  ValMDA    ValMAPE  ValMAE    PXcorr  RXcorr Secs    TTlMins')\n",
    "\n",
    "    # Outputs a summary line for each sequence in each epoch\n",
    "    if verbosity == 2:\n",
    "        print('EPc  Seq#   TrLoss    TrMSE     PredR   ActRt   ValLoss   VAdjC    VPred   VAct      VPredRt   VActRt   RunMDA')            \n",
    "                            \n",
    "    \n",
    "#    train_losses = []\n",
    "#    val_adj_close_price = []\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    min_grads = []\n",
    "    \n",
    "    # If the number of epochs is high, just sample the gradients stochastically to avoid retaining too much\n",
    "    # data.\n",
    "    if (epochs * train_length) > 1000:\n",
    "        sample_grads = 1000.0 / (epochs * train_length)\n",
    "    else:\n",
    "        sample_grads = 1.0\n",
    "\n",
    "    start_time = time.time()        \n",
    "\n",
    "    # How many times through the entire training set\n",
    "    for i in range(epochs):\n",
    "        train_losses = []\n",
    "        val_adj_close_price = []        \n",
    "        train_mses = []\n",
    "        val_mses = []\n",
    "        val_pred_rt = []\n",
    "        val_act_rt = []\n",
    "        val_pred_price = []\n",
    "        val_act_price = []        \n",
    "        running_mda = 0\n",
    "        epoch_start_time = time.time()\n",
    "        # Each sample in the training set\n",
    "        for j in range(train_start, train_start + train_length):      \n",
    "\n",
    "            # Reset the learning rate each time for the inner loop\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "            \n",
    "            t_mse_list = []\n",
    "            v_mse_list = []\n",
    "            il_ave_grads = []\n",
    "            il_max_grads = []\n",
    "            \n",
    "#            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "#            my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = dr)\n",
    "            \n",
    "#            if j%dump_log_interval == 0:\n",
    "#                dump_log = 1\n",
    "#            else:\n",
    "            # Disable inner loop instrumentations for now\n",
    "            dump_log = 0\n",
    "            \n",
    "            # How many times we train on each sample as we go\n",
    "            for iter in range(iter_per_seq):\n",
    "                # Send input / label data to Cuda if available\n",
    "                inputs = train_seq[j].to(device)\n",
    "                labels = train_labels[j].to(device)\n",
    "        \n",
    "                model.train()\n",
    "            \n",
    "                # Initialise hidden states before every training event\n",
    "                h = model.init_hidden(batch_size)  \n",
    "                                    \n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                y_pred, h, lstm_out = model(inputs, h)\n",
    "            \n",
    "                if dump_log:\n",
    "                    tp = y_pred[-1].item()\n",
    "                    ta = labels[-1].item()\n",
    "                    t_mse = (tp-ta)**2\n",
    "\n",
    "                # y_pred and labels are both size T - sequence to sequence loss function\n",
    "                train_loss = loss_function(y_pred, labels)\n",
    "            \n",
    "                # Compute gradients and update the model\n",
    "                train_loss.backward()\n",
    "                                \n",
    "                optimizer.step()\n",
    "\n",
    "                my_lr_scheduler.step()\n",
    "                \n",
    "                # Do an inner loop validation\n",
    "                if dump_log:\n",
    "                    inputs = train_seq[j+1].to(device)\n",
    "                    labels = train_labels[j+1].to(device)\n",
    "            \n",
    "                    with torch.no_grad():\n",
    "                        h = model.init_hidden(batch_size)\n",
    "                        y_pred, h, lstm_out = model(inputs, h)\n",
    "                \n",
    "                        vp = y_pred[-1].item()\n",
    "                        va = labels[-1].item()\n",
    "                        v_mse = (vp-va)**2\n",
    "                \n",
    "                    # write detailed training log\n",
    "                    with open(basefn + 'train_inner.csv', 'a', newline='') as myfile:        \n",
    "                        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                        wr.writerow([j,iter,train_loss.item(),tp,ta,t_mse,vp,va,v_mse])\n",
    "                    \n",
    "                    t_mse_list.append(t_mse)\n",
    "                    v_mse_list.append(v_mse)\n",
    "\n",
    "                    # Grab the inner loop grads - only if we're plotting as we go\n",
    "                    if plot >= 3:                    \n",
    "                        il_ave_grad = []\n",
    "                        il_max_grad = []\n",
    "                        for n,p in model.named_parameters():\n",
    "                            if (p.requires_grad) and (\"bias\" not in n):\n",
    "                                il_ave_grad.append(p.grad.abs().mean())\n",
    "                                il_max_grad.append(p.grad.abs().max())\n",
    "                        il_ave_grads.append(il_ave_grad)\n",
    "                        il_max_grads.append(il_max_grad)\n",
    "            \n",
    "            # After the inner loop completes, plot the inner-loop charts if enabled\n",
    "            if dump_log and plot >= 3:\n",
    "                plot_losses(t_mse_list, v_mse_list, title='Train / Validation Loss. Seq:'+str(j), basefn=basefn, display_plot=1, save_plot=0)\n",
    "                print(\"last tp:\",tp,\"\\tlast ta:\",ta,\"\\tlast t_mse:\",t_mse)\n",
    "                print(\"last vp:\",vp,\"\\tlast va:\",va,\"\\tlast v_mse:\",v_mse)\n",
    "            \n",
    "                plot_gradients(model, il_ave_grads, il_max_grads, basefn=basefn, display_plot=1, save_plot=0)\n",
    "\n",
    "            # Sample stochastically the last set of gradients after the inner loop for plotting later\n",
    "            if plot and (random.random() < sample_grads):\n",
    "                ave_grad=[]\n",
    "                max_grad=[]\n",
    "                min_grad=[]\n",
    "                for n,p in model.named_parameters():\n",
    "                    if (p.requires_grad) and (\"bias\" not in n):\n",
    "                        ave_grad.append(p.grad.abs().mean())\n",
    "                        max_grad.append(p.grad.abs().max())\n",
    "                        min_grad.append(p.grad.abs().min())\n",
    "                ave_grads.append(ave_grad)\n",
    "                max_grads.append(max_grad)\n",
    "                min_grads.append(min_grad)            \n",
    "            \n",
    "            # Capture the (snapshot) metrics from the last inner loop cycle\n",
    "            training_loss = train_loss.item()  # This is the sequence to sequence training loss                        \n",
    "            train_adj_close = inputs[-1][-1].item()   # Adj close at t=0\n",
    "            train_pred = y_pred[-1].item()  # Next day predicted Adj close\n",
    "            train_truth = labels[-1].item()  # Next day actual Adj close\n",
    "            train_mse = (train_pred - train_truth)**2   # MSE of just the predicted value vs truth\n",
    "            train_predicted_return = (denormalise_value(train_pred, scale, offset) / denormalise_value(train_adj_close, scale, offset)) - 1            \n",
    "            train_actual_return = (denormalise_value(train_truth, scale, offset) / denormalise_value(train_adj_close, scale, offset)) - 1\n",
    "\n",
    "            # Outer loop validation at the end of inner loop training - we always do this regardless of what\n",
    "            # validation we did in the inner loop (which is usually sampled)\n",
    "            model.eval()\n",
    "            \n",
    "            inputs = train_seq[j+1].to(device)\n",
    "            labels = train_labels[j+1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                h = model.init_hidden(batch_size)\n",
    "        \n",
    "                y_pred, h, lstm_out = model(inputs, h)\n",
    "                    \n",
    "                valid_adj_close = inputs[-1][-1].item()                \n",
    "                valid_pred = y_pred[-1].item()\n",
    "                valid_truth = labels[-1].item()\n",
    "                valid_loss = (valid_pred - valid_truth)**2  # Just the mse of the valid predicted value vs truth\n",
    "            \n",
    "                valid_predicted_return = (denormalise_value(valid_pred, scale, offset) / denormalise_value(valid_adj_close, scale, offset)) - 1\n",
    "                valid_actual_return = (denormalise_value(valid_truth, scale, offset) / denormalise_value(valid_adj_close, scale, offset)) - 1\n",
    "\n",
    "                if (valid_predicted_return * valid_actual_return >= 0):\n",
    "                    running_mda += 1                \n",
    "                \n",
    "                # If enabled, print results line at the end of inner loop training on each sequence\n",
    "                if (verbosity==2):\n",
    "                    print('{:4}'.format(i),\n",
    "                            '{:4}'.format(j), \n",
    "                          '  {:1.1e}'.format(training_loss),\n",
    "                          '  {:1.1e}'.format(train_mse),\n",
    "                          ' {: 2.3f}'.format(100.0*train_predicted_return), \n",
    "                          ' {: 2.3f}'.format(100.0*train_actual_return),\n",
    "                          '  {:1.1e}'.format(valid_loss), \n",
    "                          ' {: 1.4f}'.format(valid_adj_close), \n",
    "                          ' {: 1.4f}'.format(valid_pred), \n",
    "                          ' {: 1.4f}'.format(valid_truth),\n",
    "                          ' {: 2.4f}'.format(100.0*valid_predicted_return), \n",
    "                          ' {: 2.4f}'.format(100.0*valid_actual_return),\n",
    "                          '  {:2.2f}'.format(100.0*running_mda/(j+1)))\n",
    "                    \n",
    "                # Keep lists of the results of train / validation for each sequence for passing back up the stack\n",
    "                train_losses.append(training_loss)\n",
    "                train_mses.append(train_mse)\n",
    "                val_mses.append(valid_loss)\n",
    "                val_pred_rt.append(valid_predicted_return)\n",
    "                val_act_rt.append(valid_actual_return)\n",
    "                val_pred_price.append(valid_pred)\n",
    "                val_act_price.append(valid_truth)\n",
    "                val_adj_close_price.append(valid_adj_close)\n",
    "                \n",
    "            # End of outer loop                                        \n",
    "           \n",
    "        elapsed_secs = time.time()-start_time\n",
    "        \n",
    "        # We report stats on the last portion of the training\n",
    "        stats_len = int(0.5 * len(train_losses))\n",
    "            \n",
    "        metrics_dict = compute_metrics(np.array(val_pred_rt[-stats_len:]),\n",
    "                                   np.array(val_act_rt[-stats_len:]),\n",
    "                                   np.array(val_pred_price[-stats_len:]),\n",
    "                                   np.array(val_act_price[-stats_len:]))\n",
    "\n",
    "        # This is the results we'll pass back up the stack for the entire training\n",
    "        result_dict = {'trStats_N':stats_len, 'AvTrLoss':np.mean(train_losses[-stats_len:]),\n",
    "                   'trAvTrMSE':np.mean(train_mses[-stats_len:]),\n",
    "                   'trAvValMSE':np.mean(val_mses[-stats_len:]),\n",
    "                   'trMdnTrMSE':np.median(train_mses[-stats_len:]),\n",
    "                   'trMdnValMSE':np.median(val_mses[-stats_len:]),\n",
    "                   'train_mses':train_mses, 'valid_mses':val_mses, 'val_pred_rt':val_pred_rt, 'val_act_rt':val_act_rt, \n",
    "                   'val_pred_price':val_pred_price, 'val_act_price':val_act_price, \n",
    "                   'val_adj_close_price': val_adj_close_price,\n",
    "                    'trMDA': metrics_dict['MDA'],\n",
    "                    'trMAPE': metrics_dict['MAPE'],\n",
    "                    'trMAE': metrics_dict['MAE'],\n",
    "                    'trPXCORR': metrics_dict['PXCORR'],\n",
    "                    'trRXCORR': metrics_dict['RXCORR'],\n",
    "                    'epoch_train_time':time.time()-epoch_start_time,\n",
    "                   'train_time':elapsed_secs}\n",
    "        \n",
    "#        result_dict.update(metrics_dict)\n",
    "        \n",
    "        # Output summary stats at end of outer loop training  \n",
    "        if (verbosity == 1):\n",
    "            print('{:4}'.format(i),'{:4.2e}'.format(result_dict['AvTrLoss']),\n",
    "                          ' {:1.2e}'.format(result_dict['trAvTrMSE']),\n",
    "                          ' {:4.2e}'.format(result_dict['trAvValMSE']), \n",
    "                          ' {:1.2e}'.format(result_dict['trMdnTrMSE']),\n",
    "                          ' {:4.2e}'.format(result_dict['trMdnValMSE']),                  \n",
    "                          '  {:2.2f}'.format(result_dict['trMDA']), \n",
    "                          '    {:2.2f}'.format(result_dict['trMAPE']), \n",
    "                          '    {:2.2e}'.format(result_dict['trMAE']),\n",
    "                          '{: 1.2f}'.format(result_dict['trPXCORR']), \n",
    "                          '  {: 1.2f}'.format(result_dict['trRXCORR']),      \n",
    "                          '  {:2.2f}'.format(result_dict['epoch_train_time']),                  \n",
    "                          '  {:3.1f}'.format(result_dict['train_time']/60.0))\n",
    "            \n",
    "        with open(basefn + 'train.csv', 'a', newline='') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow([i, result_dict['AvTrLoss'],\n",
    "                        result_dict['trAvTrMSE'],\n",
    "                        result_dict['trAvValMSE'],\n",
    "                        result_dict['trMdnTrMSE'],\n",
    "                        result_dict['trMdnValMSE'],\n",
    "                        result_dict['trMDA'],\n",
    "                        result_dict['trMAPE'],\n",
    "                        result_dict['trMAE'],\n",
    "                        result_dict['trPXCORR'],\n",
    "                        result_dict['trRXCORR'],\n",
    "                        result_dict['epoch_train_time'],\n",
    "                        result_dict['train_time']/60.0,\n",
    "                        ])\n",
    "        \n",
    "        # Periodically perform a trading validation \n",
    "        if i%trading_validation_period == 0:\n",
    "            D,Q,E,A,C = generate_initial_allocation_policy(result_dict, percentiles=percentiles, scale=scale, offset=offset, basefn=basefn, verbosity=0)\n",
    "            \n",
    "            # Save both the model and the initial allocation policy so we can recreate for validation / test after training\n",
    "            # (pick the best model out of a complete training run after analysing the data)\n",
    "            \n",
    "            # Make a results directory. If the 'directory' doesn't exist, create it\n",
    "            if os.path.isdir(basefn + '/saved_models') == False:\n",
    "                os.mkdir(basefn + '/saved_models')\n",
    "            \n",
    "            torch.save(model,basefn + '/saved_models/epoch' + str(i) + '.mdl')\n",
    "            \n",
    "            with open(basefn + '/saved_models/DQEAC_dict' + str(i) + '.pkl','wb') as fp:\n",
    "                pickle.dump({'D':D, 'Q':Q, 'E':E, 'A':A, 'C':C}, fp, protocol=pickle.HIGHEST_PROTOCOL)            \n",
    "\n",
    "            # Note - prior to version 15, I wasn't deep copying the D,Q,E,A which means they would have been being updated\n",
    "            # by trading validation.\n",
    "            trial_results = []\n",
    "            for trial in range(10):\n",
    "                # For each trial make a deep copy of things that get modified by the validation process. Make sure that \n",
    "                # the model being trained and the allocation policy components are not being modified and we always start\n",
    "                # validation from the same starting point.\n",
    "                validation_model = copy.deepcopy(model)\n",
    "                copy_D = copy.deepcopy(D)\n",
    "                copy_Q = copy.deepcopy(Q)\n",
    "                copy_E = copy.deepcopy(E)\n",
    "                copy_A = copy.deepcopy(A)\n",
    "                \n",
    "                validation_model, copy_D, copy_Q, copy_E, copy_A, journal, test_result_dict = test_trading_system(validation_model, valid_seq, valid_labels, copy_D, copy_Q, copy_E, copy_A, percentiles, basefn=basefn+'/saved_models/'+str(i)+'_'+str(trial), scale=scale, offset=offset, iter_per_seq=iter_per_seq, lr = 0.001, dr = 1, verbosity=0)\n",
    "                \n",
    "                trial_results.append(test_result_dict)                \n",
    "\n",
    "            \n",
    "            mean_trial_MDA = np.mean([v['MDA'] for v in trial_results])\n",
    "            mean_trial_MSE = np.mean([v['MSE'] for v in trial_results])\n",
    "            mean_trial_MAE = np.mean([v['MAE'] for v in trial_results])\n",
    "            mean_trial_MAPE = np.mean([v['MAPE'] for v in trial_results])\n",
    "            mean_trial_RXCORR = np.mean([v['RXCORR'] for v in trial_results])\n",
    "            mean_trial_num_trades = np.mean([v['num_trades'] for v in trial_results])\n",
    "            trial_returns = [v['trade_return'] for v in trial_results]\n",
    "            mean_trial_trade_return = np.mean(trial_returns)\n",
    "            median_trial_trade_return = np.median(trial_returns)\n",
    "            min_trial_trade_return = np.min(trial_returns)\n",
    "            max_trial_trade_return = np.max(trial_returns)\n",
    "            range_trial_trade_return = max_trial_trade_return - min_trial_trade_return\n",
    "            std_trial_trade_return = np.std(trial_returns)\n",
    "            \n",
    "            print('Trading Validation - ',\n",
    "                'MDA:{:2.2f}'.format(mean_trial_MDA),\n",
    "                '  Returns Correl:{:2.2f}'.format(mean_trial_RXCORR),\n",
    "                '  Num Trades:{:4}'.format(mean_trial_num_trades),\n",
    "                '  Return%:{:2.2f}'.format(mean_trial_trade_return))\n",
    "            \n",
    "            with open(basefn + 'trade_validation.csv', 'a', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow([i,mean_trial_MDA, mean_trial_MSE,\n",
    "                            mean_trial_MAE, mean_trial_MAPE,\n",
    "                            mean_trial_RXCORR, mean_trial_num_trades, \n",
    "                            mean_trial_trade_return, median_trial_trade_return,\n",
    "                            min_trial_trade_return, max_trial_trade_return,\n",
    "                            range_trial_trade_return, std_trial_trade_return])\n",
    "\n",
    "    if plot>=1:\n",
    "        plot_gradients(model, ave_grads, max_grads, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plotly_gradients(model, min_grads, max_grads, ave_grads, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_hidden(h, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_lstm_output(lstm_out, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_weights(model, basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_losses(train_mses, val_mses, title='Model Training: Training Loss vs Validation Loss', basefn=basefn, display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_pred_rt, val_act_rt, title='Model Training: Predicted vs Actual Returns', basefn=basefn+'both_', display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_pred_rt, val_pred_rt, title='Model Training: Predicted Returns', basefn=basefn+'pred_', display_plot=plot, save_plot=1)\n",
    "        plot_returns_hist(val_act_rt, val_act_rt, title='Model Training: Actual Returns', basefn=basefn+'act_', display_plot=plot, save_plot=1)    \n",
    "            \n",
    "    return model, result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "val_results = []\n",
    "for i in range(10):\n",
    "    print(\"\\nTrial:\", i)\n",
    "    print(\"********\\n\")\n",
    "    model, journal, result = run_hypers(verbosity = 1, plot = 1)\n",
    "    test_results.append(result['test_return_pct'])\n",
    "    val_results.append(result['val_return_pct'])\n",
    "    \n",
    "print(np.mean(np.array(val_results)))\n",
    "print(np.std(np.array(val_results)))\n",
    "\n",
    "print(np.mean(np.array(test_results)))\n",
    "print(np.std(np.array(test_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"short\")\n",
    "\n",
    "for name in model.named_parameters():\n",
    "    print(name[0])\n",
    "    print(name[1][:])\n",
    "\n",
    "print(model.lstm.weight_ih_l0.size())\n",
    "print(model.lstm.bias_ih_l0.size())\n",
    "print(model.lstm.weight_hh_l0.size())\n",
    "print(model.lstm.bias_hh_l0.size())\n",
    "\n",
    "\n",
    "print(model.lstm.weight_hh_l1.size())\n",
    "print(model.lstm.weight_ih_l1.size())\n",
    "\n",
    "print(model.lstm.weight_hh_l2.size())\n",
    "print(model.lstm.weight_ih_l2.size())\n",
    "\n",
    "print(model.linear.weight.size())\n",
    "print(model.linear.bias.size())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first asset in list of assets as the source\n",
    "assets = ['SPY', 'DIA', 'ONEQ', 'IWM']\n",
    "\n",
    "print(asset_list)\n",
    "syn_df = asset_dict[asset_list[0]].copy()\n",
    "\n",
    "display(syn_df.head(5))\n",
    "print(syn_df.describe())    \n",
    "\n",
    "asset_dict['SYN'] = syn_df \n",
    "\n",
    "trend_list = [(1, 20)]\n",
    "make_synthetic_df(asset_dict['SYN'], trend_list, 0, 0, 1.4)\n",
    "\n",
    "display(syn_df.head(5))\n",
    "print(syn_df.describe())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# norm_img=cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "# plt.imshow(img, cmap='gray')\n",
    "fig, ax = plt.subplots(1,6,figsize=(20,12))\n",
    "ax[0].imshow(model.lstm.weight_ih_l0.cpu().detach().numpy(), cmap='inferno')\n",
    "ax[1].imshow(model.lstm.weight_hh_l0.cpu().detach().numpy(), cmap='inferno')\n",
    "ax[2].imshow(model.lstm.weight_ih_l1.cpu().detach().numpy(), cmap='inferno')\n",
    "ax[3].imshow(model.lstm.weight_hh_l1.cpu().detach().numpy(), cmap='inferno')\n",
    "ax[4].imshow(model.lstm.weight_ih_l2.cpu().detach().numpy(), cmap='inferno')\n",
    "ax[5].imshow(model.lstm.weight_hh_l2.cpu().detach().numpy(), cmap='inferno')\n",
    "\n",
    "# ih_l0 - has dimension 6 x 512\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock market indices\n",
    "# assets = ['^GSPC', '^DJI', '^IXIC', '^RUT']\n",
    "\n",
    "assets = ['SPY', 'DIA', 'ONEQ', 'IWM']\n",
    "\n",
    "asset_list = assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through index list, check if we already have the data (load it) or go get it from Yahoo! (save it)\n",
    "# Make a dictionary of dataframes containing all the data\n",
    "\n",
    "start_date = '01-01-2005'\n",
    "# stop_date = '05-01-2018' # First version of paper\n",
    "stop_date = '12-20-2019' # Final version of paper\n",
    "directory = '../../../data/yahoo_data'\n",
    "\n",
    "# If the 'directory' doesn't exist, create it\n",
    "#if os.path.isdir(directory) == False:\n",
    "#    os.mkdir(directory)\n",
    "\n",
    "asset_dict = {}\n",
    "for asset in asset_list:\n",
    "    df = get_data(asset_name=asset, directory=directory, start_date=start_date, stop_date=stop_date)    \n",
    "    asset_dict[asset] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset in asset_list:\n",
    "    display(asset_dict[asset].head(5))\n",
    "    print(asset, asset_dict[asset].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the assets in Plotly interactive charts with a Candlestick chart\n",
    "for asset in asset_list:\n",
    "    plotly_candlestick(asset_dict, asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we split the data into train, validation and test sets let's add the Prev Adj Close feature and\n",
    "# remove the Date and Volume columns (which are not used)\n",
    "\n",
    "for asset in asset_list:\n",
    "    asset_dict[asset] = add_features_to_df(asset_dict[asset])       \n",
    "    display(asset, asset_dict[asset].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates for each of the sets - start and end dates are inclusive\n",
    "\n",
    "# Training set\n",
    "train_start = '01-01-2005'\n",
    "train_end = '01-01-2008'\n",
    "\n",
    "# Validation set\n",
    "valid_start = '01-02-2008'\n",
    "valid_end = '12-31-2009'\n",
    "\n",
    "# A combined train / valid set\n",
    "train_val_start = '01-01-2005'\n",
    "train_val_end = '12-31-2009'\n",
    "\n",
    "# Pre-test set (120 days prior to test period, to generate allocation policy for testing)\n",
    "pre_test_start = '07-15-2009'\n",
    "pre_test_end = '12-31-2009'\n",
    "\n",
    "# Out of sample test set\n",
    "test_start = '01-01-2010'\n",
    "test_end = '12-20-2019'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_baseline = {'SPY':136.4, 'DIA':136.6, 'ONEQ':228.9, 'IWM':163.5} # Original paper\n",
    "paper_baseline = {'SPY':188.87, 'DIA':172.87, 'ONEQ':293.32, 'IWM':167.34, 'SYN':0} # Latest paper - table 12\n",
    "\n",
    "\n",
    "train_asset_dict = {}\n",
    "valid_asset_dict = {}\n",
    "test_asset_dict = {}\n",
    "train_val_asset_dict = {}\n",
    "pre_test_asset_dict = {}\n",
    "\n",
    "for asset in asset_list:\n",
    "    # Make a dataframe for this asset\n",
    "    df = asset_dict[asset]\n",
    "    \n",
    "    # Mask off the training data and assign result to a train_asset dictionary\n",
    "    mask = (df['Date'] >= train_start) & (df['Date'] <= train_end)\n",
    "    train_asset_dict[asset] = df.loc[mask]\n",
    "    \n",
    "    # Mask off the validation data and assign result to a valid_asset dictionary\n",
    "    mask = (df['Date'] >= valid_start) & (df['Date'] <= valid_end)\n",
    "    valid_asset_dict[asset] = df.loc[mask]\n",
    "    \n",
    "    # Mask off the test data and assign result to a test_asset dictionary\n",
    "    mask = (df['Date'] >= test_start) & (df['Date'] <= test_end)\n",
    "    test_asset_dict[asset] = df.loc[mask]\n",
    "    \n",
    "    # Mask off the combined train / validation data and assign result to a train_val_asset dictionary\n",
    "    mask = (df['Date'] >= train_val_start) & (df['Date'] <= train_val_end)\n",
    "    train_val_asset_dict[asset] = df.loc[mask]\n",
    "\n",
    "    # Mask off the pre_test data and assign result to a pre_test_asset dictionary\n",
    "    mask = (df['Date'] >= pre_test_start) & (df['Date'] <= pre_test_end)\n",
    "    pre_test_asset_dict[asset] = df.loc[mask]\n",
    "    \n",
    "    # Reset the indices to start from 0 again for each of the dataframes\n",
    "    train_asset_dict[asset].index = np.arange(train_asset_dict[asset].shape[0])    \n",
    "    valid_asset_dict[asset].index = np.arange(valid_asset_dict[asset].shape[0])    \n",
    "    test_asset_dict[asset].index = np.arange(test_asset_dict[asset].shape[0])\n",
    "    train_val_asset_dict[asset].index = np.arange(train_val_asset_dict[asset].shape[0])    \n",
    "    pre_test_asset_dict[asset].index = np.arange(pre_test_asset_dict[asset].shape[0])    \n",
    "\n",
    "    \n",
    "#    display(test_asset_dict[asset])\n",
    "\n",
    "print(\"Adjusted Close to Adjusted Close:\")\n",
    "print('Asset\\tStart Date\\t\\tBeg$\\tEnd Date\\t\\tEnd$\\tGain%\\tPaper%')\n",
    "for asset in asset_list:\n",
    "    # Print the buy and hold returns over the entire test period (the papers baseline)\n",
    "    start = test_asset_dict[asset]['Adj Close'].loc[0]\n",
    "    end = test_asset_dict[asset]['Adj Close'].loc[test_asset_dict[asset].shape[0]-1]\n",
    "    gain = 100.0 * (end - start) / start\n",
    "    \n",
    "    print(asset, '\\t', test_asset_dict[asset]['Date'].loc[0], '\\t{:1.2f}'.format(start), '\\t', \n",
    "          test_asset_dict[asset]['Date'].loc[test_asset_dict[asset].shape[0]-1], \n",
    "          '\\t{:1.2f}'.format(end), '\\t{:1.2f}'.format(gain), '\\t{:1.2f}'.format(paper_baseline[asset]))\n",
    "\n",
    "print(\"\\nClose to Close:\")\n",
    "print('Asset\\tStart Date\\t\\tBeg$\\tEnd Date\\t\\tEnd$\\tGain%\\tPaper%')\n",
    "for asset in asset_list:\n",
    "    # Print the buy and hold returns over the entire test period (the papers baseline)\n",
    "    start = test_asset_dict[asset]['Close'].loc[0]\n",
    "    end = test_asset_dict[asset]['Close'].loc[test_asset_dict[asset].shape[0]-1]\n",
    "    gain = 100.0 * (end - start) / start\n",
    "    \n",
    "    print(asset, '\\t', test_asset_dict[asset]['Date'].loc[0], '\\t{:1.2f}'.format(start), '\\t', \n",
    "          test_asset_dict[asset]['Date'].loc[test_asset_dict[asset].shape[0]-1], \n",
    "          '\\t{:1.2f}'.format(end), '\\t{:1.2f}'.format(gain), '\\t{:1.2f}'.format(paper_baseline[asset]))\n",
    "    \n",
    "print(\"\\nOpen to Close:\")\n",
    "print('Asset\\tStart Date\\t\\tBeg$\\tEnd Date\\t\\tEnd$\\tGain%\\tPaper%')\n",
    "for asset in asset_list:\n",
    "    # Print the buy and hold returns over the entire test period (the papers baseline)\n",
    "    start = test_asset_dict[asset]['Open'].loc[0]\n",
    "    end = test_asset_dict[asset]['Close'].loc[test_asset_dict[asset].shape[0]-1]\n",
    "    gain = 100.0 * (end - start) / start\n",
    "    \n",
    "    print(asset, '\\t', test_asset_dict[asset]['Date'].loc[0], '\\t{:1.2f}'.format(start), '\\t', \n",
    "          test_asset_dict[asset]['Date'].loc[test_asset_dict[asset].shape[0]-1], \n",
    "          '\\t{:1.2f}'.format(end), '\\t{:1.2f}'.format(gain), '\\t{:1.2f}'.format(paper_baseline[asset]))    \n",
    "    \n",
    "# It looks like the paper used Close-Close or Open-Close rather than Adjusted Close to compute gains over the test period. \n",
    "# They are a lot closer to the paper quoted returns and better correlated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set length:\", len(train_asset_dict['SPY']))\n",
    "print(\"Valid set length\", len(valid_asset_dict['SPY']))\n",
    "print(\"Train / Valid set length\", len(train_val_asset_dict['SPY']))\n",
    "print(\"Pre-test set\", len(pre_test_asset_dict['SPY']))\n",
    "print(\"Test set\",len(test_asset_dict['SPY']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
